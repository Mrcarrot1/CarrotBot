<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.Legacy</name>
    </assembly>
    <members>
        <member name="M:Microsoft.ML.Runtime.AssemblyRegistration.LoadStandardAssemblies">
            <summary>
            Loads all the assemblies in the Microsoft.ML package that contain components.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.Experiment">
            <summary>
            This class represents an entry point graph.
            The nodes in the graph represent entry point calls and
            the edges of the graph are variables that help connect the nodes.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.Experiment.Compile">
            <summary>
            Parses the nodes to determine the validity of the graph and
            to determine the inputs and outputs of the graph.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.DefaultsAutoMlEngine">
            <summary>
            AutoML engine that returns learners with default settings.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.RocketAutoMlEngine">
            <summary>
            AutoML engine that consists of distinct, hierarchical stages of operation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RocketAutoMlEngine.TopKLearners">
            <summary>
            Number of learners to retain for second stage.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RocketAutoMlEngine.SecondRoundTrialsPerLearner">
            <summary>
            Number of trials for retained second stage learners.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RocketAutoMlEngine.RandomInitialization">
            <summary>
            Use random initialization only.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RocketAutoMlEngine.NumInitializationPipelines">
            <summary>
            Number of initilization pipelines, used for random initialization only.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.UniformRandomAutoMlEngine">
            <summary>
            AutoML engine using uniform random sampling.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase">
            <summary>
            State of an AutoML search and search space.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase.Metric">
            <summary>
            Supported metric for evaluator.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase.Engine">
            <summary>
            AutoML engine (pipeline optimizer) that generates next candidates.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase.TrainerKind">
            <summary>
            Kind of trainer for task, such as binary classification trainer, multiclass trainer, etc.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase.TerminatorArgs">
            <summary>
            Arguments for creating terminator, which determines when to stop search.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AutoMlStateAutoMlStateBase.RequestedLearners">
            <summary>
            Learner set to sweep over (if available).
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.DartBoosterParameterFunction">
            <summary>
            Dropouts meet Multiple Additive Regresion Trees. See https://arxiv.org/abs/1505.01866
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.DropRate">
            <summary>
            Drop ratio for trees. Range:(0,1).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.MaxDrop">
            <summary>
            Max number of dropped tree in a boosting round.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.SkipDrop">
            <summary>
            Probability for not perform dropping in a boosting round.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.XgboostDartMode">
            <summary>
            True will enable xgboost dart mode.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.UniformDrop">
            <summary>
            True will enable uniform drop.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.UnbalancedSets">
            <summary>
            Use for binary classification when classes are not balanced.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.MinSplitGain">
            <summary>
            Minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.MaxDepth">
            <summary>
            Maximum depth of a tree. 0 means no limit. However, tree still grows by best-first.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.MinChildWeight">
            <summary>
            Minimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.SubsampleFreq">
            <summary>
            Subsample frequency. 0 means no subsample. If subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And the subset will be updated on every Subsample iteratinos.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.Subsample">
            <summary>
            Subsample ratio of the training instance. Setting it to 0.5 means that LightGBM randomly collected half of the data instances to grow trees and this will prevent overfitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.FeatureFraction">
            <summary>
            Subsample ratio of columns when constructing each tree. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.RegLambda">
            <summary>
            L2 regularization term on weights, increasing this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.RegAlpha">
            <summary>
            L1 regularization term on weights, increase this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.DartBoosterParameterFunction.ScalePosWeight">
            <summary>
            Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases).
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.GbdtBoosterParameterFunction">
            <summary>
            Traditional Gradient Boosting Decision Tree.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.UnbalancedSets">
            <summary>
            Use for binary classification when classes are not balanced.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.MinSplitGain">
            <summary>
            Minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.MaxDepth">
            <summary>
            Maximum depth of a tree. 0 means no limit. However, tree still grows by best-first.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.MinChildWeight">
            <summary>
            Minimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.SubsampleFreq">
            <summary>
            Subsample frequency. 0 means no subsample. If subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And the subset will be updated on every Subsample iteratinos.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.Subsample">
            <summary>
            Subsample ratio of the training instance. Setting it to 0.5 means that LightGBM randomly collected half of the data instances to grow trees and this will prevent overfitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.FeatureFraction">
            <summary>
            Subsample ratio of columns when constructing each tree. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.RegLambda">
            <summary>
            L2 regularization term on weights, increasing this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.RegAlpha">
            <summary>
            L1 regularization term on weights, increase this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GbdtBoosterParameterFunction.ScalePosWeight">
            <summary>
            Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases).
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.GossBoosterParameterFunction">
            <summary>
            Gradient-based One-Side Sampling.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.TopRate">
            <summary>
            Retain ratio for large gradient instances.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.OtherRate">
            <summary>
            Retain ratio for small gradient instances.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.UnbalancedSets">
            <summary>
            Use for binary classification when classes are not balanced.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.MinSplitGain">
            <summary>
            Minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.MaxDepth">
            <summary>
            Maximum depth of a tree. 0 means no limit. However, tree still grows by best-first.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.MinChildWeight">
            <summary>
            Minimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.SubsampleFreq">
            <summary>
            Subsample frequency. 0 means no subsample. If subsampleFreq > 0, it will use a subset(ratio=subsample) to train. And the subset will be updated on every Subsample iteratinos.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.Subsample">
            <summary>
            Subsample ratio of the training instance. Setting it to 0.5 means that LightGBM randomly collected half of the data instances to grow trees and this will prevent overfitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.FeatureFraction">
            <summary>
            Subsample ratio of columns when constructing each tree. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.RegLambda">
            <summary>
            L2 regularization term on weights, increasing this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.RegAlpha">
            <summary>
            L1 regularization term on weights, increase this value will make model more conservative.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GossBoosterParameterFunction.ScalePosWeight">
            <summary>
            Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FixedPlattCalibratorCalibratorTrainer.Slope">
            <summary>
            The slope parameter of f(x) = 1 / (1 + exp(-slope * x + offset)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FixedPlattCalibratorCalibratorTrainer.Offset">
            <summary>
            The offset parameter of f(x) = 1 / (1 + exp(-slope * x + offset)
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.PlattCalibratorCalibratorTrainer">
            <summary>
            Platt calibration.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.ExpLossClassificationLossFunction">
            <summary>
            Exponential loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.ExpLossClassificationLossFunction.Beta">
            <summary>
            Beta (dilation)
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.HingeLossClassificationLossFunction">
            <summary>
            Hinge loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.HingeLossClassificationLossFunction.Margin">
            <summary>
            Margin value
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.LogLossClassificationLossFunction">
            <summary>
            Log loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SmoothedHingeLossClassificationLossFunction">
            <summary>
            Smoothed Hinge loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.SmoothedHingeLossClassificationLossFunction.SmoothingConst">
            <summary>
            Smoothing constant
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.GLEarlyStoppingCriterion">
            <summary>
            Stop in case of loss of generality.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.GLEarlyStoppingCriterion.Threshold">
            <summary>
            Threshold in range [0,1].
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.LPEarlyStoppingCriterion">
            <summary>
            Stops in case of low progress.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.LPEarlyStoppingCriterion.Threshold">
            <summary>
            Threshold in range [0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.LPEarlyStoppingCriterion.WindowSize">
            <summary>
            The window size.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.PQEarlyStoppingCriterion">
            <summary>
            Stops in case of generality to progress ration exceeds threshold.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.PQEarlyStoppingCriterion.Threshold">
            <summary>
            Threshold in range [0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.PQEarlyStoppingCriterion.WindowSize">
            <summary>
            The window size.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.TREarlyStoppingCriterion">
            <summary>
            Stop if validation score exceeds threshold value.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.TREarlyStoppingCriterion.Threshold">
            <summary>
            Tolerance threshold. (Non negative value)
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.UPEarlyStoppingCriterion">
            <summary>
            Stops in case of consecutive loss in generality.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.UPEarlyStoppingCriterion.WindowSize">
            <summary>
            The window size.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.StackingEnsembleBinaryOutputCombiner.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.WeightedAverageEnsembleBinaryOutputCombiner.WeightageName">
            <summary>
            The metric type to be used to find the weights for each model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorEnsembleBinarySubModelSelector.DiversityMetricType">
            <summary>
            The metric type to be used to find the diversity among base learners
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorEnsembleBinarySubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorEnsembleBinarySubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorEnsembleBinarySubModelSelector.MetricName">
            <summary>
            The metric type to be used to find the best performance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorEnsembleBinarySubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorEnsembleBinarySubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RandomFeatureSelectorEnsembleFeatureSelector.FeaturesSelectionProportion">
            <summary>
            The proportion of features to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.MultiAverageEnsembleMulticlassOutputCombiner.Normalize">
            <summary>
            Whether to normalize the output of base models before combining them
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.MultiMedianEnsembleMulticlassOutputCombiner.Normalize">
            <summary>
            Whether to normalize the output of base models before combining them
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.MultiStackingEnsembleMulticlassOutputCombiner.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.MultiWeightedAverageEnsembleMulticlassOutputCombiner.WeightageName">
            <summary>
            The metric type to be used to find the weights for each model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.MultiWeightedAverageEnsembleMulticlassOutputCombiner.Normalize">
            <summary>
            Whether to normalize the output of base models before combining them
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorMultiClassEnsembleMulticlassSubModelSelector.DiversityMetricType">
            <summary>
            The metric type to be used to find the diversity among base learners
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorMultiClassEnsembleMulticlassSubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorMultiClassEnsembleMulticlassSubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorMultiClassEnsembleMulticlassSubModelSelector.MetricName">
            <summary>
            The metric type to be used to find the best performance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorMultiClassEnsembleMulticlassSubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceSelectorMultiClassEnsembleMulticlassSubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RegressionStackingEnsembleRegressionOutputCombiner.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorRegressionEnsembleRegressionSubModelSelector.DiversityMetricType">
            <summary>
            The metric type to be used to find the diversity among base learners
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorRegressionEnsembleRegressionSubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestDiverseSelectorRegressionEnsembleRegressionSubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceRegressionSelectorEnsembleRegressionSubModelSelector.MetricName">
            <summary>
            The metric type to be used to find the best performance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceRegressionSelectorEnsembleRegressionSubModelSelector.LearnersSelectionProportion">
            <summary>
            The proportion of best base learners to be selected. The range is 0.0-1.0
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BestPerformanceRegressionSelectorEnsembleRegressionSubModelSelector.ValidationDatasetProportion">
            <summary>
            The proportion of instances to be selected to test the individual base learner. If it is 0, it uses training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.AllInstanceSelectorEnsembleSubsetSelector.FeatureSelector">
            <summary>
            The Feature selector
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.BootstrapSelectorEnsembleSubsetSelector.FeatureSelector">
            <summary>
            The Feature selector
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.RandomPartitionSelectorEnsembleSubsetSelector.FeatureSelector">
            <summary>
            The Feature selector
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer">
            <summary>
            Uses a logit-boost boosted tree learner to perform binary classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.UnbalancedSets">
            <summary>
            Should we use derivatives optimized for unbalanced sets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeBinaryClassificationFastTreeTrainer.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer">
            <summary>
            Trains gradient boosted decision trees to the LambdaRank quasi-gradient.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.TrainDcg">
            <summary>
            Train DCG instead of NDCG
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.SortingAlgorithm">
            <summary>
            The sorting algorithm to use for DCG and LambdaMart calculations [DescendingStablePessimistic/DescendingStable/DescendingReverse/DescendingDotNet]
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.LambdaMartMaxTruncation">
            <summary>
            max-NDCG truncation to use in the Lambda Mart algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.ShiftedNdcg">
            <summary>
            Use shifted NDCG
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.CostFunctionParam">
            <summary>
            Cost function parameter (w/c)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.DistanceWeight2">
            <summary>
            Distance weight 2 adjustment to cost
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NormalizeQueryLambdas">
            <summary>
            Normalize query lambdas
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRankingFastTreeTrainer.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer">
            <summary>
            Trains gradient boosted decision trees to fit target values using least-squares.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeRegressionFastTreeTrainer.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer">
            <summary>
            Trains gradient boosted decision trees to fit target values using a Tweedie loss function. This learner is a generalization of Poisson, compound Poisson, and gamma regression.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Index">
            <summary>
            Index parameter for the Tweedie distribution, in the range [1, 2]. 1 is Poisson loss, 2 is gamma loss, and intermediate values are compound Poisson loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.FastTreeTweedieRegressionFastTreeTrainer.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.NGramNgramExtractor">
            <summary>
            Extracts NGrams from text and convert them to vector using dictionary.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramNgramExtractor.NgramLength">
            <summary>
            Ngram length
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramNgramExtractor.SkipLength">
            <summary>
            Maximum number of tokens to skip when constructing an ngram
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramNgramExtractor.AllLengths">
            <summary>
            Whether to include all ngram lengths up to NgramLength or only NgramLength
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramNgramExtractor.MaxNumTerms">
            <summary>
            Maximum number of ngrams to store in the dictionary
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramNgramExtractor.Weighting">
            <summary>
            The weighting criteria
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.NGramHashNgramExtractor">
            <summary>
            Extracts NGrams from text and convert them to vector using hashing trick.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.NgramLength">
            <summary>
            Ngram length
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.SkipLength">
            <summary>
            Maximum number of tokens to skip when constructing an ngram
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.HashBits">
            <summary>
            Number of bits to hash into. Must be between 1 and 30, inclusive.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.Seed">
            <summary>
            Hashing seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.Ordered">
            <summary>
            Whether the position of each source column should be included in the hash (when there are multiple source columns).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.InvertHash">
            <summary>
            Limit the number of keys used to generate the slot name to this many. 0 means no invert hashing, -1 means no limit.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.NGramHashNgramExtractor.AllLengths">
            <summary>
            Whether to include all ngram lengths up to ngramLength or only ngramLength
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SingleParallelLightGBM">
            <summary>
            Single node machine learning process.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SingleParallelTraining">
            <summary>
            Single node machine learning process.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.ParquetPathParserPartitionedPathParser">
            <summary>
            Extract name/value pairs from Parquet formatted directory names. Example path: Year=2018/Month=12/data1.parquet
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.PartitionedFileLoaderColumn.Name">
            <summary>
            Name of the column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.PartitionedFileLoaderColumn.Type">
            <summary>
            Data type of the column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.PartitionedFileLoaderColumn.Source">
            <summary>
            Index of the directory representing this column.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SimplePathParserPartitionedPathParser">
            <summary>
            A simple parser that extracts directory names as column values. Column names are defined as arguments.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.SimplePathParserPartitionedPathParser.Columns">
            <summary>
            Column definitions used to override the Partitioned Path Parser. Expected with the format name:type:numeric-source, e.g. col=MyFeature:R4:1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.SimplePathParserPartitionedPathParser.Type">
            <summary>
            Data type of each column.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.PoissonLossRegressionLossFunction">
            <summary>
            Poisson loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SquaredLossRegressionLossFunction">
            <summary>
            Squared loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.TweedieLossRegressionLossFunction">
            <summary>
            Tweedie loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.TweedieLossRegressionLossFunction.Index">
            <summary>
            Index parameter for the Tweedie distribution, in the range [1, 2]. 1 is Poisson loss, 2 is gamma loss, and intermediate values are compound Poisson loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.HingeLossSDCAClassificationLossFunction">
            <summary>
            Hinge loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.HingeLossSDCAClassificationLossFunction.Margin">
            <summary>
            Margin value
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.LogLossSDCAClassificationLossFunction">
            <summary>
            Log loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SmoothedHingeLossSDCAClassificationLossFunction">
            <summary>
            Smoothed Hinge loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.SmoothedHingeLossSDCAClassificationLossFunction.SmoothingConst">
            <summary>
            Smoothing constant
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.SquaredLossSDCARegressionLossFunction">
            <summary>
            Squared loss.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.IterationLimitedSearchTerminator">
            <summary>
            Terminators a sweep based on total number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.IterationLimitedSearchTerminator.FinalHistoryLength">
            <summary>
            Total number of iterations.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.CustomStopWordsRemover">
            <summary>
            Remover with list of stopwords specified by the user.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.CustomStopWordsRemover.Stopword">
            <summary>
            List of stopwords
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.PredefinedStopWordsRemover">
            <summary>
            Remover with predefined list of stop words.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.CodeGen.GeneratorBase.Generate(Microsoft.ML.Runtime.Internal.Utilities.IndentingTextWriter,System.String,System.String,Microsoft.ML.Runtime.ComponentCatalog.LoadableClassInfo,System.String,System.String,System.String,System.String,System.String,System.String,System.String,System.String,System.Collections.Generic.HashSet{System.String},System.Collections.Generic.HashSet{System.String})">
            <summary>
            Generate the module and its implementation.
            </summary>
            <param name="writer">The writer.</param>
            <param name="prefix">The module prefix.</param>
            <param name="regenerate">The command string used to generate.</param>
            <param name="component">The component.</param>
            <param name="moduleId"></param>
            <param name="moduleName"></param>
            <param name="moduleOwner"></param>
            <param name="moduleVersion"></param>
            <param name="moduleState"></param>
            <param name="moduleType"></param>
            <param name="moduleDeterminism"></param>
            <param name="moduleCategory"></param>
            <param name="exclude">The set of parameters to exclude</param>
            <param name="namespaces">The set of extra namespaces</param>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.CodeGen.ImplGeneratorBase.GenerateFieldsOrProperties(Microsoft.ML.Runtime.Internal.Utilities.IndentingTextWriter,Microsoft.ML.Runtime.CommandLine.CmdParser.ArgInfo.Arg,System.String,System.Action{Microsoft.ML.Runtime.Internal.Utilities.IndentingTextWriter,System.String,System.String,System.String,System.Boolean,System.String})">
            <summary>
            Generates private fields and public properties for all the fields in the arguments.
            Recursively generate fields and properties for subcomponents.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.CrossValidationBinaryMacro">
            <summary>
            This macro entry point implements cross validation for binary classification.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.CrossValidationMacro">
            <summary>
            This macro entry point implements cross validation.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.CVSplit">
            <summary>
            The module that splits the input dataset into the specified number of cross-validation folds, and outputs the 'training'
            and 'testing' portion of the input for each fold.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.FeatureCombiner.PrepareFeatures(Microsoft.ML.Runtime.IHostEnvironment,Microsoft.ML.Runtime.EntryPoints.FeatureCombiner.FeatureCombinerInput)">
            <summary>
            Given a list of feature columns, creates one "Features" column.
            It converts all the numeric columns to R4.
            For Key columns, it uses a KeyToValue+Term+KeyToVector transform chain to create one-hot vectors.
            The last transform is to concatenate all the resulting columns into one "Features" column.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.ImportTextData">
            <summary>
            A component for importing text files as <see cref="T:Microsoft.ML.Runtime.Data.IDataView"/>.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.ExecuteGraphCommand.SavePredictorModel(Microsoft.ML.Runtime.EntryPoints.IPredictorModel,System.String)">
            <summary>
            Saves the PredictorModel to the given path
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.ExecuteGraphCommand.SaveDataView(Microsoft.ML.Runtime.Data.IDataView,System.String,System.String)">
            <summary>
            Saves the IDV to file based on its extension
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner">
             <summary>
             This class runs a graph of entry points with the specified inputs, and produces the specified outputs.
             The entry point graph is provided as a <see cref="T:Newtonsoft.Json.Linq.JArray"/> of graph nodes. The inputs need to be provided separately:
             the graph runner will only compile a list of required inputs, and the calling code is expected to set them prior
             to running the graph.
            
             REVIEW: currently, the graph is executed synchronously, one node at a time. This is an implementation choice, we
             probably need to consider parallel asynchronous execution, once we agree on an acceptable syntax for it.
             </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner.RunAll">
            <summary>
            Run all nodes in the graph.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner.GetOutput``1(System.String)">
            <summary>
            Retrieve an output of the experiment graph.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner.GetOutputOrDefault``1(System.String)">
            <summary>
            Get the value of an EntryPointVariable present in the graph, or returns null.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner.SetInput``1(System.String,``0)">
            <summary>
            Set the input of the experiment graph.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.GraphRunner.GetPortDataKind(System.String)">
            <summary>
            Get the data kind of a particular port.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.JsonUtils.JsonManifestUtils">
            <summary>
            Utilities to generate JSON manifests for entry points and other components.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.JsonManifestUtils.BuildAllManifests(Microsoft.ML.Runtime.IExceptionContext,Microsoft.ML.Runtime.ComponentCatalog)">
            <summary>
            Builds a JSON representation of all entry points and components of the <paramref name="catalog"/>.
            </summary>
            <param name="ectx">The exception context to use</param>
            <param name="catalog">The module catalog</param>
        </member>
        <member name="M:Microsoft.ML.Runtime.EntryPoints.JsonUtils.JsonManifestUtils.BuildComponentToken(Microsoft.ML.Runtime.IExceptionContext,Microsoft.ML.Runtime.IComponentFactory,Microsoft.ML.Runtime.ComponentCatalog)">
             <summary>
             Build a token for component default value. This will look up the component in the catalog, and if it finds an entry, it will
             build a JSON structure that would be parsed into the default value.
            
             This is an inherently fragile setup in case when the factory is not trivial, but it will work well for 'property bag' factories
             that we are currently using.
             </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.MacroUtils.TrainerKinds">
            <summary>
            Lists the types of trainer signatures. Used by entry points and autoML system
            to know what types of evaluators to use for the train test / pipeline sweeper.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.EntryPoints.OneVersusAllMacro">
            <summary>
            This macro entrypoint implements OVA.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.EntryPointTransformOutput.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.EntryPointTransformOutput.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Runtime.EntryPointTrainerOutput.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.AssemblyLoadingUtils.LoadAndRegister(Microsoft.ML.Runtime.IHostEnvironment,System.String[])">
            <summary>
            Make sure the given assemblies are loaded and that their loadable classes have been catalogued.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.AssemblyLoadingUtils.LoadAssembly(Microsoft.ML.Runtime.IHostEnvironment,System.String)">
            <summary>
            Given an assembly path, load the assembly and register it with the ComponentCatalog.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Runtime.AssemblyLoadingUtils.CanContainComponents(System.Reflection.Assembly)">
            <summary>
            Checks whether <paramref name="assembly"/> references the assembly containing LoadableClassAttributeBase,
            and therefore can contain components.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.CustomTextLoader">
            <summary>
            Import a dataset from a text file
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.CustomTextLoader.InputFile">
            <summary>
            Location of the input file
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.CustomTextLoader.CustomSchema">
            <summary>
            Custom schema to use for parsing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.CustomTextLoader.Output.Data">
            <summary>
            The resulting data view
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.DataViewReference">
            <summary>
            Pass dataview from memory to experiment
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.DataViewReference.Data">
            <summary>
            Pointer to IDataView in memory
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.DataViewReference.Output.Data">
            <summary>
            The resulting data view
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.IDataViewArrayConverter">
            <summary>
            Create an array variable of IDataView
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.IDataViewArrayConverter.Data">
            <summary>
            The data sets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.IDataViewArrayConverter.Output.OutputData">
            <summary>
            The data set array
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.PredictorModelArrayConverter">
            <summary>
            Create an array variable of IPredictorModel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.PredictorModelArrayConverter.Model">
            <summary>
            The models
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.PredictorModelArrayConverter.Output.OutputModel">
            <summary>
            The model array
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.Min">
            <summary>
            First index in the range
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.Max">
            <summary>
            Last index in the range
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.AutoEnd">
            <summary>
            This range extends to the end of the line, but should be a fixed number of items
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.VariableEnd">
            <summary>
            This range extends to the end of the line, which can vary from line to line
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.AllOther">
            <summary>
            This range includes only other indices not specified
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderRange.ForceVector">
            <summary>
            Force scalar columns to be treated as vectors of length one
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.TextLoaderRange.#ctor(System.Int32)">
            <summary>
            Convenience constructor for the scalar case, when a given column
            in the schema spans only a single column in the dataset.
            <see cref="P:Microsoft.ML.Legacy.Data.TextLoaderRange.Min"/> and <see cref="P:Microsoft.ML.Legacy.Data.TextLoaderRange.Max"/> are set to the single value <paramref name="ordinal"/>.
            </summary>
            <param name="ordinal">Column index in the dataset.</param>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.TextLoaderRange.#ctor(System.Int32,System.Int32)">
            <summary>
            Convenience constructor for the vector case, when a given column
            in the schema spans contiguous columns in the dataset.
            </summary>
            <param name="min">Starting column index in the dataset.</param>
            <param name="max">Ending column index in the dataset.</param>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.KeyRange.Min">
            <summary>
            First index in the range
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.KeyRange.Max">
            <summary>
            Last index in the range
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.KeyRange.Contiguous">
            <summary>
            Whether the key is contiguous
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderColumn.Name">
            <summary>
            Name of the column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderColumn.Type">
            <summary>
            Type of the items in the column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderColumn.Source">
            <summary>
            Source index range(s) of the column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderColumn.KeyRange">
            <summary>
            For a key column, this defines the range of values
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.UseThreads">
            <summary>
            Use separate parsing threads?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.HeaderFile">
            <summary>
            File containing a header with feature names. If specified, header defined in the data file (header+) is ignored.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.MaxRows">
            <summary>
            Maximum number of rows to produce
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.AllowQuoting">
            <summary>
            Whether the input may include quoted values, which can contain separator characters, colons, and distinguish empty values from missing values. When true, consecutive separators denote a missing value and an empty value is denoted by "". When false, consecutive separators denote an empty value.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.AllowSparse">
            <summary>
            Whether the input may include sparse representations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.InputSize">
            <summary>
            Number of source columns in the text data. Default is that sparse rows contain their size information.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.Separator">
            <summary>
            Source column separator.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.Column">
            <summary>
            Column groups. Each group is specified as name:type:numeric-ranges, eg, col=Features:R4:1-17,26,35-40
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.TrimWhitespace">
            <summary>
            Remove trailing whitespace from lines
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoaderArguments.HasHeader">
            <summary>
            Data file has header with feature names. Header is read only if options 'hs' and 'hf' are not specified.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.TextLoader">
            <summary>
            Import a dataset from a text file
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoader.InputFile">
            <summary>
            Location of the input file
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoader.Arguments">
            <summary>
            Arguments
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TextLoader.Output.Data">
            <summary>
            The resulting data view
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.TextLoader.CreateFrom``1(System.Boolean,System.Char,System.Boolean,System.Boolean,System.Boolean)">
            <summary>
            Construct a TextLoader object by inferencing the dataset schema from a type.
            </summary>
            <param name="useHeader">Does the file contains header?</param>
            <param name="separator">Column separator character. Default is '\t'</param>
            <param name="allowQuotedStrings">Whether the input may include quoted values,
            which can contain separator characters, colons,
            and distinguish empty values from missing values. When true, consecutive separators
            denote a missing value and an empty value is denoted by \"\".
            When false, consecutive separators denote an empty value.</param>
            <param name="supportSparse">Whether the input may include sparse representations e.g.
            if one of the row contains "5 2:6 4:3" that's mean there are 5 columns all zero
            except for 3rd and 5th columns which have values 6 and 3</param>
            <param name="trimWhitespace">Remove trailing whitespace from lines</param>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.TextLoader.TryGetDataKind(System.Type,Microsoft.ML.Legacy.Data.DataKind@)">
            <summary>
            Try to map a System.Type to a corresponding DataKind value.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.TransformModelArrayConverter">
            <summary>
            Create an array variable of ITransformModel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TransformModelArrayConverter.TransformModel">
            <summary>
            The models
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Data.TransformModelArrayConverter.Output.OutputModel">
            <summary>
            The model array
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Data.CollectionDataSource">
            <summary>
            Creates data source for pipeline based on provided collection of data.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.CollectionDataSource.Create``1(System.Collections.Generic.IList{``0})">
            <summary>
            Creates pipeline data source. Support shuffle.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Data.CollectionDataSource.Create``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            Creates pipeline data source which can't be shuffled.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator">
            <summary>
            Evaluates an anomaly detection scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.K">
            <summary>
            Expected number of false positives
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.P">
            <summary>
            Expected false positive rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.NumTopResults">
            <summary>
            Number of top-scored predictions to display
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.Stream">
            <summary>
            Whether to calculate metrics in one pass
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.MaxAucExamples">
            <summary>
            The number of samples to use for AUC calculation. If 0, AUC is not computed. If -1, the whole dataset is used
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyDetectionEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.AnomalyPipelineEnsemble">
            <summary>
            Combine anomaly detection models into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyPipelineEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyPipelineEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.AnomalyPipelineEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator">
            <summary>
            Evaluates a binary classification scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.ProbabilityColumn">
            <summary>
            Probability column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Threshold">
            <summary>
            Probability value for classification thresholding
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.UseRawScoreThreshold">
            <summary>
            Use raw score value instead of probability for classification thresholding
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.NumRocExamples">
            <summary>
            The number of samples to use for p/r curve generation. Specify 0 for no p/r curve generation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.MaxAucExamples">
            <summary>
            The number of samples to use for AUC calculation. If 0, AUC is not computed. If -1, the whole dataset is used
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.NumAuPrcExamples">
            <summary>
            The number of samples to use for AUPRC calculation. Specify 0 for no AUPRC calculation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.BinaryClassificationEvaluator.Evaluate(Microsoft.ML.Legacy.PredictionModel,Microsoft.ML.Legacy.ILearningPipelineLoader)">
            <summary>
            Computes the quality metrics for the PredictionModel using the specified data set.
            </summary>
            <param name="model">
            The trained PredictionModel to be evaluated.
            </param>
            <param name="testData">
            The test data that will be predicted and used to evaluate the model.
            </param>
            <returns>
            A BinaryClassificationMetrics instance that describes how well the model performed against the test data.
            </returns>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationBinaryMacroSubGraphInput.Data">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationBinaryMacroSubGraphOutput.Model">
            <summary>
            The model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryCrossValidator">
            <summary>
            Cross validation for binary classification
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Data">
            <summary>
            The data set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Nodes">
            <summary>
            The training subgraph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Inputs">
            <summary>
            The training subgraph inputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Outputs">
            <summary>
            The training subgraph outputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.StratificationColumn">
            <summary>
            Column to use for stratification
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.NumFolds">
            <summary>
            Number of folds in k-fold cross-validation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryCrossValidator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryEnsemble">
            <summary>
            Combine binary classifiers into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryEnsemble.ValidatePipelines">
            <summary>
            Whether to validate that all the pipelines are identical
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryPipelineEnsemble">
            <summary>
            Combine binary classification models into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryPipelineEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryPipelineEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryPipelineEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClassificationEvaluator">
            <summary>
            Evaluates a multi class classification scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.OutputTopKAcc">
            <summary>
            Output top-K accuracy.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.NumTopClassesToOutput">
            <summary>
            Output top-K classes.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.NumClassesConfusionMatrix">
            <summary>
            Maximum number of classes in confusion matrix.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.OutputPerClassStatistics">
            <summary>
            Output per class statistics and confusion matrix.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.ClassificationEvaluator.Evaluate(Microsoft.ML.Legacy.PredictionModel,Microsoft.ML.Legacy.ILearningPipelineLoader)">
            <summary>
            Computes the quality metrics for the multi-class classification PredictionModel
            using the specified data set.
            </summary>
            <param name="model">
            The trained multi-class classification PredictionModel to be evaluated.
            </param>
            <param name="testData">
            The test data that will be predicted and used to evaluate the model.
            </param>
            <returns>
            A ClassificationMetrics instance that describes how well the model performed against the test data.
            </returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClusterEvaluator">
            <summary>
            Evaluates a clustering scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.FeatureColumn">
            <summary>
            Features column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.CalculateDbi">
            <summary>
            Calculate DBI? (time-consuming unsupervised metric)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.NumTopClustersToOutput">
            <summary>
            Output top K clusters
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.ClusterEvaluator.Evaluate(Microsoft.ML.Legacy.PredictionModel,Microsoft.ML.Legacy.ILearningPipelineLoader)">
            <summary>
            Computes the quality metrics for the PredictionModel using the specified data set.
            </summary>
            <param name="model">
            The trained PredictionModel to be evaluated.
            </param>
            <param name="testData">
            The test data that will be predicted and used to evaluate the model.
            </param>
            <returns>
            A ClusterMetrics instance that describes how well the model performed against the test data.
            </returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner">
            <summary>
            Combine the metric data views returned from cross validation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.OverallMetrics">
            <summary>
            Overall metrics datasets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.PerInstanceMetrics">
            <summary>
            Per instance metrics datasets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.ConfusionMatrix">
            <summary>
            Confusion matrix datasets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Warnings">
            <summary>
            Warning datasets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.LabelColumn">
            <summary>
            The label column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.GroupColumn">
            <summary>
            Column to use for grouping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.NameColumn">
            <summary>
            Name column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Kind">
            <summary>
            Specifies the trainer kind, which determines the evaluator to be used.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationResultsCombiner.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationMacroSubGraphInput.Data">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationMacroSubGraphOutput.PredictorModel">
            <summary>
            The predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidationMacroSubGraphOutput.TransformModel">
            <summary>
            The transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.CrossValidator">
            <summary>
            Cross validation for general learning
            </summary>
            <summary>
            Performs cross-validation on a pipeline.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Data">
            <summary>
            The data set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.TransformModel">
            <summary>
            The transform model from the pipeline before this command. It gets included in the Output.PredictorModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Nodes">
            <summary>
            The training subgraph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Inputs">
            <summary>
            The training subgraph inputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Outputs">
            <summary>
            The training subgraph outputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.StratificationColumn">
            <summary>
            Column to use for stratification
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.NumFolds">
            <summary>
            Number of folds in k-fold cross-validation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Kind">
            <summary>
            Specifies the trainer kind, which determines the evaluator to be used.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.GroupColumn">
            <summary>
            Column to use for grouping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.NameColumn">
            <summary>
            Name column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.PredictorModel">
            <summary>
            The final model including the trained predictor model and the model from the transforms, provided as the Input.TransformModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.TransformModel">
            <summary>
            The final model including the trained predictor model and the model from the transforms, provided as the Input.TransformModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.CrossValidator.CrossValidate``2(Microsoft.ML.Legacy.LearningPipeline)">
            <summary>
            Performs cross validation on a pipeline.
            </summary>
            <typeparam name="TInput">Class type that represents input schema.</typeparam>
            <typeparam name="TOutput">Class type that represents prediction schema.</typeparam>
            <param name="pipeline">Machine learning pipeline may contain loader, transforms and at least one trainer.</param>
            <returns>List containing metrics and predictor model for each fold</returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter">
            <summary>
            Split the dataset into the specified number of cross-validation folds (train and test sets)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter.NumFolds">
            <summary>
            Number of folds to split into
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter.StratificationColumn">
            <summary>
            Stratification column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter.Output.TrainData">
            <summary>
            Training data (one dataset per fold)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.CrossValidatorDatasetSplitter.Output.TestData">
            <summary>
            Testing data (one dataset per fold)
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.DatasetTransformer">
            <summary>
            Applies a TransformModel to a dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.DatasetTransformer.TransformModel">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.DatasetTransformer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.DatasetTransformer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.EnsembleSummary">
            <summary>
            Summarize a pipeline ensemble predictor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.EnsembleSummary.PredictorModel">
            <summary>
            The predictor to summarize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.EnsembleSummary.Output.Summaries">
            <summary>
            The summaries of the individual predictors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.EnsembleSummary.Output.Stats">
            <summary>
            The model statistics of the individual predictors
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.FixedPlattCalibrator">
            <summary>
            Apply a Platt calibrator with a fixed slope and offset to an input model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.Slope">
            <summary>
            The slope parameter of the calibration function 1 / (1 + exp(-slope * x + offset)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.Offset">
            <summary>
            The offset parameter of the calibration function 1 / (1 + exp(-slope * x + offset)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.UncalibratedPredictorModel">
            <summary>
            The predictor to calibrate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.MaxRows">
            <summary>
            The maximum number of examples to train the calibrator on
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.FixedPlattCalibrator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.MultiClassPipelineEnsemble">
            <summary>
            Combine multiclass classifiers into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiClassPipelineEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiClassPipelineEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiClassPipelineEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator">
            <summary>
            Evaluates a multi output regression scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.LossFunction">
            <summary>
            Loss function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.SupressScoresAndLabels">
            <summary>
            Supress labels and scores in per-instance outputs?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.MultiOutputRegressionEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.NaiveCalibrator">
            <summary>
            Apply a Naive calibrator to an input model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.NaiveCalibrator.UncalibratedPredictorModel">
            <summary>
            The predictor to calibrate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.NaiveCalibrator.MaxRows">
            <summary>
            The maximum number of examples to train the calibrator on
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.NaiveCalibrator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.NaiveCalibrator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAllMacroSubGraphOutput.Model">
            <summary>
            The predictor model for the subgraph exemplar.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.OneVersusAll">
            <summary>
        Trains a one-versus-all multi-class classifier on top of the specified binary classifier.
      </summary><remarks>
        <para>In this strategy, a binary classification algorithm is used to train one classifier for each class, which distinguishes that class from all other classes.
        Prediction is then performed by running these binary classifiers, and choosing the prediction with the highest confidence score.</para>
        <para>This algorithm can be used with any of the binary classifiers in ML.NET.
        A few binary classifiers already have implementation for multi-class problems,
        thus users can choose either one depending on the context.</para>
        <para>The OVA version of a binary classifier, such as wrapping a LightGbmBinaryClassifier ,
        can be different from LightGbmClassifier, which develops a multi-class classifier directly.</para>
        <para>Note that even if the classifier indicates that it does not need caching, OneVersusAll will always
        request caching, as it will be performing multiple passes over the data set.
        These learner will request normalization from the data pipeline if the classifier indicates it would benefit from it.</para>
      </remarks><seealso cref="T:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Trainers.LightGbmClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier" /><example>
        <code language="csharp">
          pipeline.Add(OneVersusAll.With(new StochasticDualCoordinateAscentBinaryClassifier()));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.Nodes">
            <summary>
            The subgraph for the binary trainer used to construct the OVA learner. This should be a TrainBinary node.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.OutputForSubGraph">
            <summary>
            The training subgraph output.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.UseProbabilities">
            <summary>
            Use probabilities in OVA combiner
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OneVersusAll.Output.PredictorModel">
            <summary>
            The trained multiclass model
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.OneVersusAll.With(Microsoft.ML.Runtime.EntryPoints.CommonInputs.ITrainerInputWithLabel,System.Boolean)">
             <summary>
             One-versus-all, OvA, learner (also known as One-vs.-rest, "OvR") is a multi-class learner
             with the strategy to fit one binary classifier per class in the dataset.
             It trains the provided binary classifier for each class against the other classes, where the current
             class is treated as the positive labels and examples in other classes are treated as the negative classes.
             See <a href="https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest">wikipedia</a> page.
             </summary>
            <example>
             In order to use it all you need to do is add it to pipeline as regular learner:
            
             pipeline.Add(OneVersusAll.With(new StochasticDualCoordinateAscentBinaryClassifier()));
             </example>
             <remarks>
             The base trainer must be a binary classifier. To check the available binary classifiers, type BinaryClassifiers,
             and look at the available binary learners as suggested by IntelliSense.
             </remarks>
             <param name="trainer">Underlying binary trainer</param>
             <param name="useProbabilities">"Use probabilities (vs. raw outputs) to identify top-score category.
             By specifying it to false, you can tell One-versus-all to not use the probabilities but instead
             the raw uncalibrated scores from each predictor. This is generally not recommended, since these quantities
             are not meant to be comparable from one predictor to another, unlike calibrated probabilities.</param>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.OnnxConverter">
            <summary>
            Converts the model to ONNX format.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.Onnx">
            <summary>
            The path to write the output ONNX to.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.Json">
            <summary>
            The path to write the output JSON to.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.Name">
            <summary>
            The 'name' property in the output ONNX. By default this will be the ONNX extension-less name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.Domain">
            <summary>
            The 'domain' property in the output ONNX.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.InputsToDrop">
            <summary>
            Array of input column names to drop
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.OutputsToDrop">
            <summary>
            Array of output column names to drop
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.Model">
            <summary>
            Model that needs to be converted to ONNX format.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OnnxConverter.DataFile">
            <summary>
            The data file
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.OnnxConverter.Convert(Microsoft.ML.Legacy.PredictionModel)">
             <summary>
             <a href="https://onnx.ai/">ONNX</a> is an intermediate representation format
             for machine learning models. It is used to make models portable such that you can
             train a model using a toolkit and run it in another tookit's runtime, for example,
             you can create a model using ML.NET, export it to an ONNX-ML model file,
             then load and run that ONNX-ML model in Windows ML, on an UWP Windows 10 app.
            
             This API converts an ML.NET model to ONNX-ML format by inspecting the transform pipeline
             from the end, checking for components that know how to save themselves as ONNX.
             The first item in the transform pipeline that does not know how to save itself
             as ONNX, is considered the "input" to the ONNX pipeline. (Ideally this would be the
             original loader itself, but this may not be possible if the user used unsavable
             transforms in defining the pipe.) All the columns in the source that are a type the
             ONNX knows how to deal with will be tracked. Intermediate transformations of the
             data appearing as new columns will appear in the output block of the ONNX, with names
             derived from the corresponding column names. The ONNX JSON will be serialized to a
             path defined through the Json option.
            
             This API supports the following arguments:
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.Onnx"/> indicates the file to write the ONNX protocol buffer file to. This is required.
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.Json"/> indicates the file to write the JSON representation of the ONNX model. This is optional.
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.Name"/> indicates the name property in the ONNX model. If left unspecified, it will
             be the extension-less name of the file specified in the onnx indicates the protocol buffer file
             to write the ONNX representation to.
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.Domain"/> indicates the domain name of the model. ONNX uses reverse domain name space indicators.
             For example com.microsoft.cognitiveservices. This is a required field.
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.InputsToDrop"/> is a string array of input column names to omit from the input mapping.
             A common scenario might be to drop the label column, for instance, since it may not be practically
             useful for the pipeline. Note that any columns depending on these naturally cannot be saved.
             <see cref="P:Microsoft.ML.Legacy.Models.OnnxConverter.OutputsToDrop"/> is similar, except for the output schema. Note that the pipeline handler
             is currently not intelligent enough to drop intermediate calculations that produce this value: this will
             merely omit that value from the actual output.
            
             Transforms that can be exported to ONNX
             1. Concat
             2. KeyToVector
             3. NAReplace
             4. Normalize
             5. Term
             6. Categorical
            
             Learners that can be exported to ONNX
             1. FastTree
             2. LightGBM
             3. Logistic Regression
            
             See <a href="https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/OnnxTests.cs"/>
             for an example on how to train a model and then convert that model to ONNX.
             </summary>
             <param name="model">Model that needs to be converted to ONNX format.</param>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.OvaModelCombiner">
            <summary>
            Combines a sequence of PredictorModels into a single model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.ModelArray">
            <summary>
            Input models
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.UseProbabilities">
            <summary>
            Use probabilities from learners instead of raw values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.OvaModelCombiner.Output.PredictorModel">
            <summary>
            Predictor model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.PAVCalibrator">
            <summary>
            Apply a PAV calibrator to an input model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PAVCalibrator.UncalibratedPredictorModel">
            <summary>
            The predictor to calibrate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PAVCalibrator.MaxRows">
            <summary>
            The maximum number of examples to train the calibrator on
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PAVCalibrator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PAVCalibrator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.PipelineSweeper">
            <summary>
            AutoML pipeline sweeping optimzation macro.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.TrainingData">
            <summary>
            The data to be used for training.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.TestingData">
            <summary>
            The data to be used for testing.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.StateArguments">
            <summary>
            The arguments for creating an AutoMlState component.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.State">
            <summary>
            The stateful object conducting of the autoML search.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.BatchSize">
            <summary>
            Number of candidate pipelines to retrieve each round.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.CandidateOutputs">
            <summary>
            Output datasets from previous iteration of sweep.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.LabelColumns">
            <summary>
            Column(s) to use as Role 'Label'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.GroupColumns">
            <summary>
            Column(s) to use as Role 'Group'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.WeightColumns">
            <summary>
            Column(s) to use as Role 'Weight'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.NameColumns">
            <summary>
            Column(s) to use as Role 'Name'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.NumericFeatureColumns">
            <summary>
            Column(s) to use as Role 'NumericFeature'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.CategoricalFeatureColumns">
            <summary>
            Column(s) to use as Role 'CategoricalFeature'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.TextFeatureColumns">
            <summary>
            Column(s) to use as Role 'TextFeature'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.ImagePathColumns">
            <summary>
            Column(s) to use as Role 'ImagePath'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.Output.State">
            <summary>
            Stateful autoML object, keeps track of where the search in progress.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PipelineSweeper.Output.Results">
            <summary>
            Results of the sweep, including pipelines (as graph strings), IDs, and metric values.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.PlattCalibrator">
            <summary>
            Apply a Platt calibrator to an input model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PlattCalibrator.UncalibratedPredictorModel">
            <summary>
            The predictor to calibrate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PlattCalibrator.MaxRows">
            <summary>
            The maximum number of examples to train the calibrator on
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PlattCalibrator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.PlattCalibrator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator">
            <summary>
            Evaluates a quantile regression scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.LossFunction">
            <summary>
            Loss function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.Index">
            <summary>
            Quantile index to select
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.QuantileRegressionEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RankerEvaluator">
            <summary>
            Evaluates a ranking scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.GroupIdColumn">
            <summary>
            Column to use for the group ID
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.DcgTruncationLevel">
            <summary>
            Maximum truncation level for computing (N)DCG
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.LabelGains">
            <summary>
            Label relevance gains
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RankerEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RegressionEnsemble">
            <summary>
            Combine regression models into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEnsemble.ValidatePipelines">
            <summary>
            Whether to validate that all the pipelines are identical
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RegressionEvaluator">
            <summary>
            Evaluates a regression scored dataset.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.LossFunction">
            <summary>
            Loss function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.LabelColumn">
            <summary>
            Column to use for labels.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.WeightColumn">
            <summary>
            Weight column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.ScoreColumn">
            <summary>
            Score column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.StratColumn">
            <summary>
            Stratification column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.Data">
            <summary>
            The data to be used for evaluation.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.NameColumn">
            <summary>
            Name column name.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.RegressionEvaluator.Evaluate(Microsoft.ML.Legacy.PredictionModel,Microsoft.ML.Legacy.ILearningPipelineLoader)">
            <summary>
            Computes the quality metrics for the PredictionModel using the specified data set.
            </summary>
            <param name="model">
            The trained PredictionModel to be evaluated.
            </param>
            <param name="testData">
            The test data that will be predicted and used to evaluate the model.
            </param>
            <returns>
            A RegressionMetrics instance that describes how well the model performed against the test data.
            </returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RegressionPipelineEnsemble">
            <summary>
            Combine regression models into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionPipelineEnsemble.ModelCombiner">
            <summary>
            The combiner used to combine the scores
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionPipelineEnsemble.Models">
            <summary>
            The models to combine into an ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionPipelineEnsemble.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.Summarizer">
            <summary>
            Summarize a linear regression predictor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.Summarizer.PredictorModel">
            <summary>
            The predictor to summarize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.Summarizer.Output.Summary">
            <summary>
            The summary of a predictor
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.Summarizer.Output.Stats">
            <summary>
            The training set statistics. Note that this output can be null.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.SweepResultExtractor">
            <summary>
            Extracts the sweep result.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.SweepResultExtractor.State">
            <summary>
            The stateful object conducting of the autoML search.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.SweepResultExtractor.Output.State">
            <summary>
            Stateful autoML object, keeps track of where the search in progress.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.SweepResultExtractor.Output.Results">
            <summary>
            Results of the sweep, including pipelines (as graph strings), IDs, and metric values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryMacroSubGraphInput.Data">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryMacroSubGraphOutput.Model">
            <summary>
            The model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator">
            <summary>
            Train test for binary classification
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.TestingData">
            <summary>
            The data to be used for testing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Nodes">
            <summary>
            The training subgraph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Inputs">
            <summary>
            The training subgraph inputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Outputs">
            <summary>
            The training subgraph outputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestBinaryEvaluator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestMacroSubGraphInput.Data">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestMacroSubGraphOutput.PredictorModel">
            <summary>
            The predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestMacroSubGraphOutput.TransformModel">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.TrainTestEvaluator">
            <summary>
            General train test for any supported evaluator
            </summary>
            <summary>
            Performs Train-Test on a pipeline.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.TestingData">
            <summary>
            The data to be used for testing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.TransformModel">
            <summary>
            The aggregated transform model from the pipeline before this command, to apply to the test data, and also include in the final model, together with the predictor model.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Nodes">
            <summary>
            The training subgraph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Inputs">
            <summary>
            The training subgraph inputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Outputs">
            <summary>
            The training subgraph outputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Kind">
            <summary>
            Specifies the trainer kind, which determines the evaluator to be used.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.PipelineId">
            <summary>
            Identifies which pipeline was run for this train test.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.IncludeTrainingMetrics">
            <summary>
            Indicates whether to include and output training dataset metrics.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.GroupColumn">
            <summary>
            Column to use for grouping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.NameColumn">
            <summary>
            Name column name
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.PredictorModel">
            <summary>
            The final model including the trained predictor model and the model from the transforms, provided as the Input.TransformModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.TransformModel">
            <summary>
            The final model including the trained predictor model and the model from the transforms, provided as the Input.TransformModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.Warnings">
            <summary>
            Warning dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.OverallMetrics">
            <summary>
            Overall metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.PerInstanceMetrics">
            <summary>
            Per instance metrics dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.ConfusionMatrix">
            <summary>
            Confusion matrix dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.TrainingWarnings">
            <summary>
            Warning dataset for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.TrainingOverallMetrics">
            <summary>
            Overall metrics dataset for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.TrainingPerInstanceMetrics">
            <summary>
            Per instance metrics dataset for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.TrainTestEvaluator.Output.TrainingConfusionMatrix">
            <summary>
            Confusion matrix dataset for training
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.Models.TrainTestEvaluator.TrainTestEvaluate``2(Microsoft.ML.Legacy.LearningPipeline,Microsoft.ML.Legacy.ILearningPipelineLoader)">
            <summary>
            Performs train-test on a pipeline.
            </summary>
            <typeparam name="TInput">Class type that represents input schema.</typeparam>
            <typeparam name="TOutput">Class type that represents prediction schema.</typeparam>
            <param name="pipeline">Machine learning pipeline that contains <see cref="T:Microsoft.ML.Legacy.ILearningPipelineLoader"/>,
            transforms and at least one trainer.</param>
            <param name="testData"><see cref="T:Microsoft.ML.Legacy.ILearningPipelineLoader"/> that represents the test dataset.</param>
            <returns>Metrics and predictor model.</returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics">
            <summary>
            This class contains the overall metrics computed by binary classification evaluators.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.Auc">
            <summary>
            Gets the area under the ROC curve.
            </summary>
            <remarks>
            The area under the ROC curve is equal to the probability that the classifier ranks
            a randomly chosen positive instance higher than a randomly chosen negative one
            (assuming 'positive' ranks higher than 'negative').
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.Accuracy">
            <summary>
            Gets the accuracy of a classifier which is the proportion of correct predictions in the test set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.PositivePrecision">
            <summary>
            Gets the positive precision of a classifier which is the proportion of correctly predicted
            positive instances among all the positive predictions (i.e., the number of positive instances
            predicted as positive, divided by the total number of instances predicted as positive).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.PositiveRecall">
            <summary>
            Gets the positive recall of a classifier which is the proportion of correctly predicted
            positive instances among all the positive instances (i.e., the number of positive instances
            predicted as positive, divided by the total number of positive instances).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.NegativePrecision">
            <summary>
            Gets the negative precision of a classifier which is the proportion of correctly predicted
            negative instances among all the negative predictions (i.e., the number of negative instances
            predicted as negative, divided by the total number of instances predicted as negative).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.NegativeRecall">
            <summary>
            Gets the negative recall of a classifier which is the proportion of correctly predicted
            negative instances among all the negative instances (i.e., the number of negative instances
            predicted as negative, divided by the total number of negative instances).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.LogLoss">
            <summary>
            Gets the log-loss of the classifier.
            </summary>
            <remarks>
            The log-loss metric, is computed as follows:
            LL = - (1/m) * sum( log(p[i]))
            where m is the number of instances in the test set.
            p[i] is the probability returned by the classifier if the instance belongs to class 1,
            and 1 minus the probability returned by the classifier if the instance belongs to class 0.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.LogLossReduction">
            <summary>
            Gets the log-loss reduction (also known as relative log-loss, or reduction in information gain - RIG)
            of the classifier.
            </summary>
            <remarks>
            The log-loss reduction is scaled relative to a classifier that predicts the prior for every example:
            (LL(prior) - LL(classifier)) / LL(prior)
            This metric can be interpreted as the advantage of the classifier over a random prediction.
            E.g., if the RIG equals 20, it can be interpreted as "the probability of a correct prediction is
            20% better than random guessing".
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.Entropy">
            <summary>
            Gets the test-set entropy (prior Log-Loss/instance) of the classifier.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.F1Score">
            <summary>
            Gets the F1 score of the classifier.
            </summary>
            <remarks>
            F1 score is the harmonic mean of precision and recall: 2 * precision * recall / (precision + recall).
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.Auprc">
            <summary>
            Gets the area under the precision/recall curve of the classifier.
            </summary>
            <remarks>
            The area under the precision/recall curve is a single number summary of the information in the
            precision/recall curve. It is increasingly used in the machine learning community, particularly
            for imbalanced datasets where one class is observed more frequently than the other. On these
            datasets, AUPRC can highlight performance differences that are lost with AUC.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.ConfusionMatrix">
            <summary>
            Gets the confusion matrix, or error matrix, of the classifier.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.RowTag">
            <summary>
            For cross-validation, this is equal to "Fold N" for per-fold metric rows, "Overall" for the average metrics and "STD" for standard deviation.
            For non-CV scenarios, this is equal to null
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.BinaryClassificationMetrics.SerializationClass">
            <summary>
            This class contains the public fields necessary to deserialize from IDataView.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClassificationMetrics">
            <summary>
            This class contains the overall metrics computed by multi-class classification evaluators.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.AccuracyMicro">
             <summary>
             Gets the micro-average accuracy of the model.
             </summary>
             <remarks>
             The micro-average is the fraction of instances predicted correctly.
            
             The micro-average metric weighs each class according to the number of instances that belong
             to it in the dataset.
             </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.AccuracyMacro">
             <summary>
             Gets the macro-average accuracy of the model.
             </summary>
             <remarks>
             The macro-average is computed by taking the average over all the classes of the fraction
             of correct predictions in this class (the number of correctly predicted instances in the class,
             divided by the total number of instances in the class).
            
             The macro-average metric gives the same weight to each class, no matter how many instances from
             that class the dataset contains.
             </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.LogLoss">
            <summary>
            Gets the average log-loss of the classifier.
            </summary>
            <remarks>
            The log-loss metric, is computed as follows:
            LL = - (1/m) * sum( log(p[i]))
            where m is the number of instances in the test set.
            p[i] is the probability returned by the classifier if the instance belongs to class 1,
            and 1 minus the probability returned by the classifier if the instance belongs to class 0.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.LogLossReduction">
            <summary>
            Gets the log-loss reduction (also known as relative log-loss, or reduction in information gain - RIG)
            of the classifier.
            </summary>
            <remarks>
            The log-loss reduction is scaled relative to a classifier that predicts the prior for every example:
            (LL(prior) - LL(classifier)) / LL(prior)
            This metric can be interpreted as the advantage of the classifier over a random prediction.
            E.g., if the RIG equals 20, it can be interpreted as "the probability of a correct prediction is
            20% better than random guessing".
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.TopKAccuracy">
            <summary>
            If <see cref="P:Microsoft.ML.Legacy.Models.ClassificationEvaluator.OutputTopKAcc"/> was specified on the
            evaluator to be k, then TopKAccuracy is the relative number of examples where
            the true label is one of the top k predicted labels by the predictor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.PerClassLogLoss">
            <summary>
            Gets the log-loss of the classifier for each class.
            </summary>
            <remarks>
            The log-loss metric, is computed as follows:
            LL = - (1/m) * sum( log(p[i]))
            where m is the number of instances in the test set.
            p[i] is the probability returned by the classifier if the instance belongs to the class,
            and 1 minus the probability returned by the classifier if the instance does not belong to the class.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.RowTag">
            <summary>
            For cross-validation, this is equal to "Fold N" for per-fold metric rows, "Overall" for the average metrics and "STD" for standard deviation.
            For non-CV scenarios, this is equal to null
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClassificationMetrics.ConfusionMatrix">
            <summary>
            Gets the confusion matrix, or error matrix, of the classifier.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClassificationMetrics.SerializationClass">
            <summary>
            This class contains the public fields necessary to deserialize from IDataView.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClusterMetrics">
            <summary>
            This class contains the overall metrics computed by cluster evaluators.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterMetrics.Dbi">
            <summary>
            Davies-Bouldin Index.
            </summary>
            <remarks>
            DBI is a measure of the how much scatter is in the cluster and the cluster separation.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterMetrics.Nmi">
            <summary>
            Normalized Mutual Information
            </summary>
            <remarks>
            NMI is a measure of the mutual dependence between the true and predicted cluster labels for instances in the dataset.
            NMI ranges between 0 and 1 where "0" indicates clustering is random and "1" indicates clustering is perfect w.r.t true labels.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterMetrics.AvgMinScore">
            <summary>
            Average minimum score.
            </summary>
            <remarks>
            AvgMinScore is the average squared-distance of examples from the respective cluster centroids.
            It is defined as
            AvgMinScore  = (1/m) * sum ((xi - c(xi))^2)
            where m is the number of instances in the dataset.
            xi is the i'th instance and c(xi) is the centriod of the predicted cluster for xi.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ClusterMetrics.RowTag">
            <summary>
            For cross-validation, this is equal to "Fold N" for per-fold metric rows, "Overall" for the average metrics and "STD" for standard deviation.
            For non-CV scenarios, this is equal to null
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ClusterMetrics.SerializationClass">
            <summary>
            This class contains the public fields necessary to deserialize from IDataView.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.ConfusionMatrix">
            <summary>
            The confusion matrix shows the predicted values vs the actual values.
            Each row of the matrix represents the instances in a predicted class
            while each column represents the instances in the actual class.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ConfusionMatrix.Order">
            <summary>
            Gets the number of rows or columns in the matrix.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ConfusionMatrix.ClassNames">
            <summary>
            Gets the class names of the confusion matrix in the same
            order as the rows/columns.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ConfusionMatrix.Item(System.Int32,System.Int32)">
            <summary>
            Obtains the value at the specified indices.
            </summary>
            <param name="x">
            The row index to retrieve.
            </param>
            <param name="y">
            The column index to retrieve.
            </param>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.ConfusionMatrix.Item(System.String,System.String)">
            <summary>
            Obtains the value for the specified class names.
            </summary>
            <param name="x">
            The name of the class for which row to retrieve.
            </param>
            <param name="y">
            The name of the class for which column to retrieve.
            </param>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RegressionMetrics">
            <summary>
            This class contains the overall metrics computed by regression evaluators.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.L1">
            <summary>
            Gets the absolute loss of the model.
            </summary>
            <remarks>
            The absolute loss is defined as
            L1 = (1/m) * sum( abs( yi - y'i))
            where m is the number of instances in the test set.
            y'i are the predicted labels for each instance.
            yi are the correct labels of each instance.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.L2">
            <summary>
            Gets the squared loss of the model.
            </summary>
            <remarks>
            The squared loss is defined as
            L2 = (1/m) * sum(( yi - y'i)^2)
            where m is the number of instances in the test set.
            y'i are the predicted labels for each instance.
            yi are the correct labels of each instance.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.Rms">
            <summary>
            Gets the root mean square loss (or RMS) which is the square root of the L2 loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.LossFn">
            <summary>
            Gets the user defined loss function.
            </summary>
            <remarks>
            This is the average of a loss function defined by the user,
            computed over all the instances in the test set.
            </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.RSquared">
            <summary>
            Gets the R squared value of the model, which is also known as
            the coefficient of determination​.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Models.RegressionMetrics.RowTag">
            <summary>
            For cross-validation, this is equal to "Fold N" for per-fold metric rows, "Overall" for the average metrics and "STD" for standard deviation.
            For non-CV scenarios, this is equal to null
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Models.RegressionMetrics.SerializationClass">
            <summary>
            This class contains the public fields necessary to deserialize from IDataView.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier">
            <summary>
        Averaged Perceptron Binary Classifier. 
      </summary><remarks>
        Perceptron is a classification algorithm that makes its predictions based on a linear function.
        I.e., for an instance with feature values f0, f1,..., f_D-1, , the prediction is given by the sign of sigma[0,D-1] ( w_i * f_i), where w_0, w_1,...,w_D-1 are the weights computed by the algorithm.
        <para>
          Perceptron is an online algorithm, i.e., it processes the instances in the training set one at a time.
          The weights are initialized to be 0, or some random values. Then, for each example in the training set, the value of sigma[0, D-1] (w_i * f_i) is computed.
          If this value has the same sign as the label of the current example, the weights remain the same. If they have opposite signs,
          the weights vector is updated by either subtracting or adding (if the label is negative or positive, respectively) the feature vector of the current example,
          multiplied by a factor 0 &lt; a &lt;= 1, called the learning rate. In a generalization of this algorithm, the weights are updated by adding the feature vector multiplied by the learning rate,
          and by the gradient of some loss function (in the specific case described above, the loss is hinge-loss, whose gradient is 1 when it is non-zero).
        </para>
        <para>
          In Averaged Perceptron (AKA voted-perceptron), the weight vectors are stored,
          together with a weight that counts the number of iterations it survived (this is equivalent to storing the weight vector after every iteration, regardless of whether it was updated or not).
          The prediction is then calculated by taking the weighted average of all the sums sigma[0, D-1] (w_i * f_i) or the different weight vectors.
        </para>
        <para> For more information see:</para>
        <para><a href="https://en.wikipedia.org/wiki/Perceptron">Wikipedia entry for Perceptron</a></para>
        <para><a href="https://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8200">Large Margin Classification Using the Perceptron Algorithm</a></para>
      </remarks>
            <!-- No matching elements were found for the following include tag --><include file="../Microsoft.ML.StandardLearners/Standard/Online/doc.xml" path="doc/members/example[@name=&quot;AP&quot;]/*" />
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.Calibrator">
            <summary>
            The calibrator kind to apply to the predictor. Specify null for no calibration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.MaxCalibrationExamples">
            <summary>
            The maximum number of examples to use when training the calibrator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.LearningRate">
            <summary>
            Learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.DecreaseLearningRate">
            <summary>
            Decrease learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.ResetWeightsAfterXExamples">
            <summary>
            Number of examples after which weights will be reset to the current average
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.DoLazyUpdates">
            <summary>
            Instead of updating averaged weights on every example, only update when loss is nonzero
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.L2RegularizerWeight">
            <summary>
            L2 Regularization Weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.RecencyGain">
            <summary>
            Extra weight given to more recent updates
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.RecencyGainMulti">
            <summary>
            Whether Recency Gain is multiplicative (vs. additive)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.Averaged">
            <summary>
            Do averaging?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.AveragedTolerance">
            <summary>
            The inexactness tolerance for averaging
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.NumIterations">
            <summary>
            Number of iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.InitialWeights">
            <summary>
            Initial Weights and bias, comma-separated
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.Shuffle">
            <summary>
            Whether to shuffle for each training iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.StreamingCacheSize">
            <summary>
            Size of cache when trained in Scope
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier">
            <summary>
            Train binary ensemble.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.SubModelSelectorType">
            <summary>
            Algorithm to prune the base learners for selective Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.OutputCombiner">
            <summary>
            Output combiner
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.NumModels">
            <summary>
            Number of models per batch. If not specified, will default to 50 if there is only one base predictor, or the number of base predictors otherwise.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.BatchSize">
            <summary>
            Batch size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.SamplingType">
            <summary>
            Sampling Type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.TrainParallel">
            <summary>
            All the base learners will run asynchronously if the value is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.ShowMetrics">
            <summary>
            True, if metrics for each model need to be evaluated and shown in comparison table. This is done by using validation set if available or the training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.EnsembleClassification">
            <summary>
            Train multiclass ensemble.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.SubModelSelectorType">
            <summary>
            Algorithm to prune the base learners for selective Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.OutputCombiner">
            <summary>
            Output combiner
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.NumModels">
            <summary>
            Number of models per batch. If not specified, will default to 50 if there is only one base predictor, or the number of base predictors otherwise.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.BatchSize">
            <summary>
            Batch size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.SamplingType">
            <summary>
            Sampling Type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.TrainParallel">
            <summary>
            All the base learners will run asynchronously if the value is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.ShowMetrics">
            <summary>
            True, if metrics for each model need to be evaluated and shown in comparison table. This is done by using validation set if available or the training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleClassification.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.EnsembleRegression">
            <summary>
            Train regression ensemble.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.SubModelSelectorType">
            <summary>
            Algorithm to prune the base learners for selective Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.OutputCombiner">
            <summary>
            Output combiner
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.NumModels">
            <summary>
            Number of models per batch. If not specified, will default to 50 if there is only one base predictor, or the number of base predictors otherwise.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.BatchSize">
            <summary>
            Batch size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.SamplingType">
            <summary>
            Sampling Type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.TrainParallel">
            <summary>
            All the base learners will run asynchronously if the value is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.ShowMetrics">
            <summary>
            True, if metrics for each model need to be evaluated and shown in comparison table. This is done by using validation set if available or the training set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.EnsembleRegression.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier">
            <summary>
        Trains a random forest to fit target values using least-squares.
      </summary><remarks>
        Decision trees are non-parametric models that perform a sequence of simple tests on inputs.
        This decision procedure maps them to outputs found in the training dataset whose inputs were similar to the instance being processed.
        A decision is made at each node of the binary tree data structure based on a measure of similarity that maps each instance recursively through the branches of the tree until the appropriate leaf node is reached and the output decision returned.
        <para>Decision trees have several advantages:</para>
        <list type="bullet">
          <item><description>They are efficient in both computation and memory usage during training and prediction. </description></item>
          <item><description>They can represent non-linear decision boundaries.</description></item>
          <item><description>They perform integrated feature selection and classification. </description></item>
          <item><description>They are resilient in the presence of noisy features.</description></item>
        </list>
        <para>Fast forest is a random forest implementation.
        The model consists of an ensemble of decision trees. Each tree in a decision forest outputs a Gaussian distribution by way of prediction.
        An aggregation is performed over the ensemble of trees to find a Gaussian distribution closest to the combined distribution for all trees in the model.
        This decision forest classifier consists of an ensemble of decision trees.</para>
        <para>Generally, ensemble models provide better coverage and accuracy than single decision trees.
         Each tree in a decision forest outputs a Gaussian distribution.</para>
         <para>For more see: </para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/Random_forest">Wikipedia: Random forest</a></description></item>
          <item><description><a href="http://jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">Quantile regression forest</a></description></item>
          <item><description><a href="https://blogs.technet.microsoft.com/machinelearning/2014/09/10/from-stumps-to-trees-to-forests/">From Stumps to Trees to Forests</a></description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          new FastForestBinaryClassifier
          {
            NumTrees = 100,
            NumLeaves = 50,
            Calibrator = new FixedPlattCalibratorCalibratorTrainer()
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Calibrator">
            <summary>
            The calibrator kind to apply to the predictor. Specify null for no calibration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxCalibrationExamples">
            <summary>
            The maximum number of examples to use when training the calibrator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.QuantileSampleCount">
            <summary>
            Number of labels to be sampled from each leaf to make the distribtuion
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastForestRegressor">
            <summary>
        Trains a random forest to fit target values using least-squares.
      </summary><remarks>
        Decision trees are non-parametric models that perform a sequence of simple tests on inputs.
        This decision procedure maps them to outputs found in the training dataset whose inputs were similar to the instance being processed.
        A decision is made at each node of the binary tree data structure based on a measure of similarity that maps each instance recursively through the branches of the tree until the appropriate leaf node is reached and the output decision returned.
        <para>Decision trees have several advantages:</para>
        <list type="bullet">
          <item><description>They are efficient in both computation and memory usage during training and prediction. </description></item>
          <item><description>They can represent non-linear decision boundaries.</description></item>
          <item><description>They perform integrated feature selection and classification. </description></item>
          <item><description>They are resilient in the presence of noisy features.</description></item>
        </list>
        <para>Fast forest is a random forest implementation.
        The model consists of an ensemble of decision trees. Each tree in a decision forest outputs a Gaussian distribution by way of prediction.
        An aggregation is performed over the ensemble of trees to find a Gaussian distribution closest to the combined distribution for all trees in the model.
        This decision forest classifier consists of an ensemble of decision trees.</para>
        <para>Generally, ensemble models provide better coverage and accuracy than single decision trees.
         Each tree in a decision forest outputs a Gaussian distribution.</para>
         <para>For more see: </para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/Random_forest">Wikipedia: Random forest</a></description></item>
          <item><description><a href="http://jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">Quantile regression forest</a></description></item>
          <item><description><a href="https://blogs.technet.microsoft.com/machinelearning/2014/09/10/from-stumps-to-trees-to-forests/">From Stumps to Trees to Forests</a></description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          new FastForestRegressor
          {
            NumTrees = 100,
            NumLeaves = 50,
            NumThreads = 5,
            EntropyCoefficient = 0.3
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.ShuffleLabels">
            <summary>
            Shuffle the labels on every iteration. Useful probably only if using this tree as a tree leaf featurizer for multiclass.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.QuantileSampleCount">
            <summary>
            Number of labels to be sampled from each leaf to make the distribtuion
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastForestRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier">
            <summary>
        Trains gradient boosted decision trees to the LambdaRank quasi-gradient. 
      </summary><remarks>
        <para>
          FastTree is an efficient implementation of the <a href="https://arxiv.org/abs/1505.01866">MART</a> gradient boosting algorithm.
          Gradient boosting is a machine learning technique for regression problems.
          It builds each regression tree in a step-wise fashion, using a predefined loss function to measure the error for each step and corrects for it in the next.
          So this prediction model is actually an ensemble of weaker prediction models. In regression problems, boosting builds a series of of such trees in a step-wise fashion and then selects the optimal tree using an arbitrary differentiable loss function.
        </para>
        <para>
          MART learns an ensemble of regression trees, which is a decision tree with scalar values in its leaves.
          A decision (or regression) tree is a binary tree-like flow chart, where at each interior node one decides which of the two child nodes to continue to based on one of the feature values from the input.
          At each leaf node, a value is returned. In the interior nodes, the decision is based on the test 'x &lt;= v' where x is the value of the feature in the input sample and v is one of the possible values of this feature.
          The functions that can be produced by a regression tree are all the piece-wise constant functions.
        </para>
        <para>
          The ensemble of trees is produced by computing, in each step, a regression tree that approximates the gradient of the loss function, and adding it to the previous tree with coefficients that minimize the loss of the new tree.
          The output of the ensemble produced by MART on a given instance is the sum of the tree outputs.
        </para>
        <list type="bullet">
          <item><description>In case of a binary classification problem, the output is converted to a probability by using some form of calibration.</description></item>
          <item><description>In case of a regression problem, the output is the predicted value of the function.</description></item>
          <item><description>In case of a ranking problem, the instances are ordered by the output value of the ensemble.</description></item>
        </list>
        <para>For more information see:</para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">Wikipedia: Gradient boosting (Gradient tree boosting).</a></description></item>
          <item><description><a href="https://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1013203451">Greedy function approximation: A gradient boosting machine.</a></description></item>
        </list>  
    </remarks>
            <example>
        <code language="csharp">
          new FastTreeBinaryClassifier
          {
            NumTrees = 100,
            EarlyStoppingRule = new PQEarlyStoppingCriterion(),
            LearningRates = 0.4f,
            DropoutRate = 0.05f
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.UnbalancedSets">
            <summary>
            Should we use derivatives optimized for unbalanced sets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastTreeRanker">
            <summary>
        Trains gradient boosted decision trees to the LambdaRank quasi-gradient. 
      </summary><remarks>
        <para>
          FastTree is an efficient implementation of the <a href="https://arxiv.org/abs/1505.01866">MART</a> gradient boosting algorithm.
          Gradient boosting is a machine learning technique for regression problems.
          It builds each regression tree in a step-wise fashion, using a predefined loss function to measure the error for each step and corrects for it in the next.
          So this prediction model is actually an ensemble of weaker prediction models. In regression problems, boosting builds a series of of such trees in a step-wise fashion and then selects the optimal tree using an arbitrary differentiable loss function.
        </para>
        <para>
          MART learns an ensemble of regression trees, which is a decision tree with scalar values in its leaves.
          A decision (or regression) tree is a binary tree-like flow chart, where at each interior node one decides which of the two child nodes to continue to based on one of the feature values from the input.
          At each leaf node, a value is returned. In the interior nodes, the decision is based on the test 'x &lt;= v' where x is the value of the feature in the input sample and v is one of the possible values of this feature.
          The functions that can be produced by a regression tree are all the piece-wise constant functions.
        </para>
        <para>
          The ensemble of trees is produced by computing, in each step, a regression tree that approximates the gradient of the loss function, and adding it to the previous tree with coefficients that minimize the loss of the new tree.
          The output of the ensemble produced by MART on a given instance is the sum of the tree outputs.
        </para>
        <list type="bullet">
          <item><description>In case of a binary classification problem, the output is converted to a probability by using some form of calibration.</description></item>
          <item><description>In case of a regression problem, the output is the predicted value of the function.</description></item>
          <item><description>In case of a ranking problem, the instances are ordered by the output value of the ensemble.</description></item>
        </list>
        <para>For more information see:</para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">Wikipedia: Gradient boosting (Gradient tree boosting).</a></description></item>
          <item><description><a href="https://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1013203451">Greedy function approximation: A gradient boosting machine.</a></description></item>
        </list>  
    </remarks>
            <example>
        <code language="csharp">
          new FastTreeRanker
          {
            SortingAlgorithm = "DescendingReverse",
            OptimizationAlgorithm = BoostedTreeArgsOptimizationAlgorithmType.AcceleratedGradientDescent
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.TrainDcg">
            <summary>
            Train DCG instead of NDCG
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.SortingAlgorithm">
            <summary>
            The sorting algorithm to use for DCG and LambdaMart calculations [DescendingStablePessimistic/DescendingStable/DescendingReverse/DescendingDotNet]
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.LambdaMartMaxTruncation">
            <summary>
            max-NDCG truncation to use in the Lambda Mart algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.ShiftedNdcg">
            <summary>
            Use shifted NDCG
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.CostFunctionParam">
            <summary>
            Cost function parameter (w/c)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.DistanceWeight2">
            <summary>
            Distance weight 2 adjustment to cost
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NormalizeQueryLambdas">
            <summary>
            Normalize query lambdas
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRanker.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastTreeRegressor">
            <summary>
        Trains gradient boosted decision trees to the LambdaRank quasi-gradient. 
      </summary><remarks>
        <para>
          FastTree is an efficient implementation of the <a href="https://arxiv.org/abs/1505.01866">MART</a> gradient boosting algorithm.
          Gradient boosting is a machine learning technique for regression problems.
          It builds each regression tree in a step-wise fashion, using a predefined loss function to measure the error for each step and corrects for it in the next.
          So this prediction model is actually an ensemble of weaker prediction models. In regression problems, boosting builds a series of of such trees in a step-wise fashion and then selects the optimal tree using an arbitrary differentiable loss function.
        </para>
        <para>
          MART learns an ensemble of regression trees, which is a decision tree with scalar values in its leaves.
          A decision (or regression) tree is a binary tree-like flow chart, where at each interior node one decides which of the two child nodes to continue to based on one of the feature values from the input.
          At each leaf node, a value is returned. In the interior nodes, the decision is based on the test 'x &lt;= v' where x is the value of the feature in the input sample and v is one of the possible values of this feature.
          The functions that can be produced by a regression tree are all the piece-wise constant functions.
        </para>
        <para>
          The ensemble of trees is produced by computing, in each step, a regression tree that approximates the gradient of the loss function, and adding it to the previous tree with coefficients that minimize the loss of the new tree.
          The output of the ensemble produced by MART on a given instance is the sum of the tree outputs.
        </para>
        <list type="bullet">
          <item><description>In case of a binary classification problem, the output is converted to a probability by using some form of calibration.</description></item>
          <item><description>In case of a regression problem, the output is the predicted value of the function.</description></item>
          <item><description>In case of a ranking problem, the instances are ordered by the output value of the ensemble.</description></item>
        </list>
        <para>For more information see:</para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">Wikipedia: Gradient boosting (Gradient tree boosting).</a></description></item>
          <item><description><a href="https://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1013203451">Greedy function approximation: A gradient boosting machine.</a></description></item>
        </list>  
    </remarks>
            <example>
        <code language="csharp">
          new FastTreeRegressor
          {
            NumTrees = 200,
            EarlyStoppingRule = new GLEarlyStoppingCriterion(),
            LearningRates = 0.4f,
            DropoutRate = 0.05f
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor">
            <summary>
        Trains gradient boosted decision trees to fit target values using a Tweedie loss function. 
        This learner is a generalization of Poisson, compound Poisson, and gamma regression.
      </summary><remarks>
        The Tweedie boosting model follows the mathematics established in <a href="https://arxiv.org/pdf/1508.06378.pdf">
        Insurance Premium Prediction via Gradient Tree-Boosted Tweedie Compound Poisson Models.</a> from Yang, Quan, and Zou. 
        <para>For an introduction to Gradient Boosting, and more information, see:</para>
        <para><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">Wikipedia: Gradient boosting (Gradient tree boosting)</a></para>
        <para><a href="https://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1013203451">Greedy function approximation: A gradient boosting machine</a></para>
      </remarks>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Index">
            <summary>
            Index parameter for the Tweedie distribution, in the range [1, 2]. 1 is Poisson loss, 2 is gamma loss, and intermediate values are compound Poisson loss.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.BestStepRankingRegressionTrees">
            <summary>
            Use best regression step trees?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.UseLineSearch">
            <summary>
            Should we use line search for a step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.NumPostBracketSteps">
            <summary>
            Number of post-bracket line search steps
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MinStepSize">
            <summary>
            Minimum line search step size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.OptimizationAlgorithm">
            <summary>
            Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.EarlyStoppingRule">
            <summary>
            Early stopping rule. (Validation set (/valid) is required.)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.EarlyStoppingMetrics">
            <summary>
            Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.UseTolerantPruning">
            <summary>
            Use window and tolerance for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.PruningThreshold">
            <summary>
            The tolerance threshold for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.PruningWindowSize">
            <summary>
            The moving window size for pruning
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Shrinkage">
            <summary>
            Shrinkage
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.DropoutRate">
            <summary>
            Dropout rate for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.WriteLastEnsemble">
            <summary>
            Write the last ensemble instead of the one determined by early stopping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MaxTreeOutput">
            <summary>
            Upper bound on absolute value of single tree output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.RandomStart">
            <summary>
            Training starts from random ordering (determined by /r1)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FilterZeroLambdas">
            <summary>
            Filter zero lambdas during training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.BaselineScoresFormula">
            <summary>
            Freeform defining the scores that should be used as the baseline ranker
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.BaselineAlphaRisk">
            <summary>
            Baseline alpha for tradeoffs of risk (0 is normal training)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.PositionDiscountFreeform">
            <summary>
            The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.ParallelTrainer">
            <summary>
            Allows to choose Parallel FastTree Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureSelectSeed">
            <summary>
            The seed of the active feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.HistogramPoolSize">
            <summary>
            The number of histograms in the pool (between 2 and numLeaves)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.CategoricalSplit">
            <summary>
            Whether to do split based on multiple categorical feature values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MaxCategoricalGroupsPerNode">
            <summary>
            Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MaxCategoricalSplitPoints">
            <summary>
            Maximum categorical split points to consider when splitting on a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MinDocsPercentageForCategoricalSplit">
            <summary>
            Minimum categorical docs percentage in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MinDocsForCategoricalSplit">
            <summary>
            Minimum categorical doc count in a bin to consider for a split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Bias">
            <summary>
            Bias for calculating gradient for each feature bin for a categorical feature.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Bundling">
            <summary>
            Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.SparsifyThreshold">
            <summary>
            Sparsity level needed to use sparse feature representation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureFirstUsePenalty">
            <summary>
            The feature first use penalty coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureReusePenalty">
            <summary>
            The feature re-use penalty (regularization) coefficient
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.SoftmaxTemperature">
            <summary>
            The temperature of the randomized softmax distribution for choosing the feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.ExecutionTimes">
            <summary>
            Print execution time breakdown to stdout
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.NumLeaves">
            <summary>
            The max number of leaves in each regression tree
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MinDocumentsInLeafs">
            <summary>
            The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.NumTrees">
            <summary>
            Total number of decision trees to create in the ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.BaggingSize">
            <summary>
            Number of trees in each bag (0 for disabling bagging)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.BaggingTrainFraction">
            <summary>
            Percentage of training examples used in each bag
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.SplitFraction">
            <summary>
            The fraction of features (chosen randomly) to use on each split
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Smoothing">
            <summary>
            Smoothing paramter for tree regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.AllowEmptyTrees">
            <summary>
            When a root split is impossible, allow training to proceed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureCompressionLevel">
            <summary>
            The level of feature compression to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.CompressEnsemble">
            <summary>
            Compress the tree Ensemble
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.MaxTreesAfterCompression">
            <summary>
            Maximum Number of trees after compression
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.PrintTestGraph">
            <summary>
            Print metrics graph for the first test set
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.PrintTrainValidGraph">
            <summary>
            Print Train and Validation metrics in graph
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.TestFrequency">
            <summary>
            Calculate metric values for train/valid/test every k rounds
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FastTreeTweedieRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier">
            <summary>
        Train a field-aware factorization machine for binary classification using ADAGRAD (an advanced stochastic gradient method). 
      </summary><remarks>
        Field Aware Factorization Machines use, in addition to the input variables, factorized parameters to model the interaction between pairs of variables.
        The algorithm is particularly useful for high dimensional datasets which can be very sparse (e.g. click-prediction for advertising systems).
        <para>An advantage of FFM over SVMs is that the training data does not need to be stored in memory, and the coefficients can be optimized directly.
          For a general idea of what Field-aware Factorization Machines are see: <a href="https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf">Field Aware Factorization Machines</a>
        </para>
        <para>See references below for more details. 
        This trainer is essentially faster the one introduced in [2] because of some implemtation tricks[3].
        </para>
          <list type="bullet">
            <item>
              <description><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf">Field-aware Factorization Machines for CTR Prediction</a></description></item>
            <item>
              <description><a href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></description>
            </item>
            <item>
              <description><a href="https://github.com/wschin/fast-ffm/blob/master/fast-ffm.pdf">An Improved Stochastic Gradient Method for Training Large-scale Field-aware Factorization Machine.</a></description>
            </item>
          </list>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new FieldAwareFactorizationMachineBinaryClassifier
          { 
            LearningRate = 0.5f, 
            Iter=2 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.LearningRate">
            <summary>
            Initial learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Iters">
            <summary>
            Number of training iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.LatentDim">
            <summary>
            Latent space dimension
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.LambdaLinear">
            <summary>
            Regularization coefficient of linear weights
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.LambdaLatent">
            <summary>
            Regularization coefficient of latent weights
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Norm">
            <summary>
            Whether to normalize the input vectors so that the concatenation of all fields' feature vectors is unit-length
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Shuffle">
            <summary>
            Whether to shuffle for each training iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Verbose">
            <summary>
            Report traning progress or not
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Radius">
            <summary>
            Radius of initial latent factors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.FieldAwareFactorizationMachineBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier">
            <summary>
            Trains a gradient boosted stump per feature, on all features simultaneously, to fit target values using least-squares. It mantains no interactions between features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.UnbalancedSets">
            <summary>
            Should we use derivatives optimized for unbalanced sets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.NumIterations">
            <summary>
            Total number of iterations over all features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.MaxOutput">
            <summary>
            Upper bound on absolute value of single output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.MinDocuments">
            <summary>
            Minimum number of training instances required to form a partition
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor">
            <summary>
            Trains a gradient boosted stump per feature, on all features simultaneously, to fit target values using least-squares. It mantains no interactions between features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.PruningMetrics">
            <summary>
            Metric for pruning. (For regression, 1: L1, 2:L2; default L2)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.EntropyCoefficient">
            <summary>
            The entropy (regularization) coefficient between 0 and 1
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.GainConfidenceLevel">
            <summary>
            Tree fitting gain confidence requirement (should be in the range [0,1) ).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.NumIterations">
            <summary>
            Total number of iterations over all features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.NumThreads">
            <summary>
            The number of threads to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.LearningRates">
            <summary>
            The learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.DiskTranspose">
            <summary>
            Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.MaxBins">
            <summary>
            Maximum number of distinct values (bins) per feature
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.MaxOutput">
            <summary>
            Upper bound on absolute value of single output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.GetDerivativesSampleRate">
            <summary>
            Sample each query 1 in k times in the GetDerivatives function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.RngSeed">
            <summary>
            The seed of the random number generator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.MinDocuments">
            <summary>
            Minimum number of training instances required to form a partition
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.FeatureFlocks">
            <summary>
            Whether to collectivize features during dataset preparation to speed up training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.EnablePruning">
            <summary>
            Enable post-training pruning to avoid overfitting. (a validation set is required)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.GeneralizedAdditiveModelRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer">
            <summary>
        K-means is a popular clustering algorithm. With K-means, the data is clustered into a specified 
        number of clusters in order to minimize the within-cluster sum of squares.
      </summary><remarks>
        K-means++ improves upon K-means by using the <a href="https://research.microsoft.com/apps/pubs/default.aspx?id=252149">Yinyang K-Means</a> method for choosing the initial cluster centers.
        YYK-Means accelerates K-Means up to an order of magnitude while producing exactly the same clustering results (modulo floating point precision issues).
        YYK-Means observes that there is a lot of redundancy across iterations in the KMeans algorithms and most points do not change their clusters during an iteration.
        It uses various bounding techniques to identify this redundancy and eliminate many distance computations and optimize centroid computations.
        <para>For more information on K-means, and K-means++ see:</para>
        <list type="bullet">
          <item><description><a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means</a></description></item>
          <item><description><a href="https://en.wikipedia.org/wiki/K-means%2b%2b">K-means++</a></description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          new KMeansPlusPlusClusterer
          {
            MaxIterations = 100,
            NumThreads = 5,
            InitAlgorithm = KMeansPlusPlusTrainerInitAlgorithm.KMeansParallel
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.K">
            <summary>
            The number of clusters
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.InitAlgorithm">
            <summary>
            Cluster initialization algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.OptTol">
            <summary>
            Tolerance parameter for trainer convergence. Lower = slower, more accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.MaxIterations">
            <summary>
            Maximum number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.AccelMemBudgetMb">
            <summary>
            Memory budget (in MBs) to use for KMeans acceleration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.NumThreads">
            <summary>
            Degree of lock-free parallelism. Defaults to automatic. Determinism not guaranteed.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.KMeansPlusPlusClusterer.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier">
            <summary>
        Trains a Light GBM Model.
      </summary><remarks>
        Light GBM is an open source implementation of boosted trees.
        <a href="https://github.com/Microsoft/LightGBM/wiki">GitHub: LightGBM</a>
      </remarks>
            <example>
        <code language="csharp">
          new LightGbmBinaryClassifier
          {
            NumBoostRound = 200,
            LearningRate = 0.5f,
            NumLeaves = 32,
            MinDataPerLeaf = 20
          }
        </code>
      </example>
            <summary>
            This API requires Microsoft.ML.LightGBM nuget.
            </summary>
            <example>
            <code>
            pipeline.Add(new LightGbmBinaryClassifier() { NumLeaves = 5, NumBoostRound = 5, MinDataPerLeaf = 2 })
            </code>
            </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.NumBoostRound">
            <summary>
            Number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.LearningRate">
            <summary>
            Shrinkage rate for trees, used to prevent over-fitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.NumLeaves">
            <summary>
            Maximum leaves for trees.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.MinDataPerLeaf">
            <summary>
            Minimum number of instances needed in a child.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.MaxBin">
            <summary>
            Max number of bucket bin for features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.Booster">
            <summary>
            Which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.VerboseEval">
            <summary>
            Verbose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.Silent">
            <summary>
            Printing running messages.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.NThread">
            <summary>
            Number of parallel threads used to run LightGBM.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.EvalMetric">
            <summary>
            Evaluation metrics.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.UseSoftmax">
            <summary>
            Use softmax loss for the multi classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.EarlyStoppingRound">
            <summary>
            Rounds of early stopping, 0 will disable it.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.Sigmoid">
            <summary>
            Parameter for the sigmoid function. Used only in LightGbmBinaryTrainer, LightGbmMulticlassTrainer and in LightGbmRankingTrainer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.BatchSize">
            <summary>
            Number of entries in a batch when loading data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.UseCat">
            <summary>
            Enable categorical split or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.UseMissing">
            <summary>
            Enable missing value auto infer or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.MinDataPerGroup">
            <summary>
            Min number of instances per categorical group.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.MaxCatThreshold">
            <summary>
            Max number of categorical thresholds.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.CatSmooth">
            <summary>
            Lapalace smooth term in categorical feature spilt. Avoid the bias of small categories.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.CatL2">
            <summary>
            L2 Regularization for categorical split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.ParallelTrainer">
            <summary>
            Parallel LightGBM Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LightGbmClassifier">
            <summary>
        Trains a Light GBM Model.
      </summary><remarks>
        Light GBM is an open source implementation of boosted trees.
        <a href="https://github.com/Microsoft/LightGBM/wiki">GitHub: LightGBM</a>
      </remarks>
            <example>
        <code language="csharp">
          new LightGbmClassifier
          {
            NumBoostRound = 200,
            LearningRate = 0.5f,
            NumLeaves = 32,
            MinDataPerLeaf = 20
          }
        </code>
      </example>
            <summary>
            This API requires Microsoft.ML.LightGBM nuget.
            </summary>
            <example>
            <code>
            pipeline.Add(new LightGbmClassifier() { NumLeaves = 5, NumBoostRound = 5, MinDataPerLeaf = 2 })
            </code>
            </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.NumBoostRound">
            <summary>
            Number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.LearningRate">
            <summary>
            Shrinkage rate for trees, used to prevent over-fitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.NumLeaves">
            <summary>
            Maximum leaves for trees.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.MinDataPerLeaf">
            <summary>
            Minimum number of instances needed in a child.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.MaxBin">
            <summary>
            Max number of bucket bin for features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.Booster">
            <summary>
            Which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.VerboseEval">
            <summary>
            Verbose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.Silent">
            <summary>
            Printing running messages.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.NThread">
            <summary>
            Number of parallel threads used to run LightGBM.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.EvalMetric">
            <summary>
            Evaluation metrics.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.UseSoftmax">
            <summary>
            Use softmax loss for the multi classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.EarlyStoppingRound">
            <summary>
            Rounds of early stopping, 0 will disable it.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.Sigmoid">
            <summary>
            Parameter for the sigmoid function. Used only in LightGbmBinaryTrainer, LightGbmMulticlassTrainer and in LightGbmRankingTrainer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.BatchSize">
            <summary>
            Number of entries in a batch when loading data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.UseCat">
            <summary>
            Enable categorical split or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.UseMissing">
            <summary>
            Enable missing value auto infer or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.MinDataPerGroup">
            <summary>
            Min number of instances per categorical group.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.MaxCatThreshold">
            <summary>
            Max number of categorical thresholds.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.CatSmooth">
            <summary>
            Lapalace smooth term in categorical feature spilt. Avoid the bias of small categories.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.CatL2">
            <summary>
            L2 Regularization for categorical split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.ParallelTrainer">
            <summary>
            Parallel LightGBM Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LightGbmRanker">
            <summary>
        Trains a Light GBM Model.
      </summary><remarks>
        Light GBM is an open source implementation of boosted trees.
        <a href="https://github.com/Microsoft/LightGBM/wiki">GitHub: LightGBM</a>
      </remarks>
            <example>
        <code language="csharp">
          new LightGbmRanker
          {
            NumBoostRound = 100,
            LearningRate = 0.5f,
            NumLeaves = 32,
            MinDataPerLeaf = 20,
            Booster = new GbdtBoosterParameterFunction
            {
              MinSplitGain = 3,
              MaxDepth = 200,
              Subsample = 0.5
            }
          }
        </code>
      </example>
            <summary>
            This API requires Microsoft.ML.LightGBM nuget.
            </summary>
            <example>
            <code>
            pipeline.Add(new LightGbmRanker() { NumLeaves = 5, NumBoostRound = 5, MinDataPerLeaf = 2 })
            </code>
            </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.NumBoostRound">
            <summary>
            Number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.LearningRate">
            <summary>
            Shrinkage rate for trees, used to prevent over-fitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.NumLeaves">
            <summary>
            Maximum leaves for trees.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.MinDataPerLeaf">
            <summary>
            Minimum number of instances needed in a child.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.MaxBin">
            <summary>
            Max number of bucket bin for features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.Booster">
            <summary>
            Which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.VerboseEval">
            <summary>
            Verbose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.Silent">
            <summary>
            Printing running messages.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.NThread">
            <summary>
            Number of parallel threads used to run LightGBM.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.EvalMetric">
            <summary>
            Evaluation metrics.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.UseSoftmax">
            <summary>
            Use softmax loss for the multi classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.EarlyStoppingRound">
            <summary>
            Rounds of early stopping, 0 will disable it.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.Sigmoid">
            <summary>
            Parameter for the sigmoid function. Used only in LightGbmBinaryTrainer, LightGbmMulticlassTrainer and in LightGbmRankingTrainer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.BatchSize">
            <summary>
            Number of entries in a batch when loading data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.UseCat">
            <summary>
            Enable categorical split or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.UseMissing">
            <summary>
            Enable missing value auto infer or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.MinDataPerGroup">
            <summary>
            Min number of instances per categorical group.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.MaxCatThreshold">
            <summary>
            Max number of categorical thresholds.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.CatSmooth">
            <summary>
            Lapalace smooth term in categorical feature spilt. Avoid the bias of small categories.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.CatL2">
            <summary>
            L2 Regularization for categorical split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.ParallelTrainer">
            <summary>
            Parallel LightGBM Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRanker.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LightGbmRegressor">
            <summary>
        Trains a Light GBM Model.
      </summary><remarks>
        Light GBM is an open source implementation of boosted trees.
        <a href="https://github.com/Microsoft/LightGBM/wiki">GitHub: LightGBM</a>
      </remarks>
            <example>
        <code language="csharp">
          new LightGbmRegressor
          {
            NumBoostRound = 100,
            LearningRate = 0.5f,
            NumLeaves = 32,
            MinDataPerLeaf = 20,
            Booster = new DartBoosterParameterFunction
            {
              XgboostDartMode = true,
              UniformDrop = true
            }
          }
        </code>
      </example>
            <summary>
            This API requires Microsoft.ML.LightGBM nuget.
            </summary>
            <example>
            <code>
            pipeline.Add(new LightGbmRegressor() { NumLeaves = 5, NumBoostRound = 5, MinDataPerLeaf = 2 })
            </code>
            </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.NumBoostRound">
            <summary>
            Number of iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.LearningRate">
            <summary>
            Shrinkage rate for trees, used to prevent over-fitting. Range: (0,1].
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.NumLeaves">
            <summary>
            Maximum leaves for trees.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.MinDataPerLeaf">
            <summary>
            Minimum number of instances needed in a child.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.MaxBin">
            <summary>
            Max number of bucket bin for features.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.Booster">
            <summary>
            Which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.VerboseEval">
            <summary>
            Verbose
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.Silent">
            <summary>
            Printing running messages.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.NThread">
            <summary>
            Number of parallel threads used to run LightGBM.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.EvalMetric">
            <summary>
            Evaluation metrics.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.UseSoftmax">
            <summary>
            Use softmax loss for the multi classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.EarlyStoppingRound">
            <summary>
            Rounds of early stopping, 0 will disable it.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.CustomGains">
            <summary>
            Comma seperated list of gains associated to each relevance label.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.Sigmoid">
            <summary>
            Parameter for the sigmoid function. Used only in LightGbmBinaryTrainer, LightGbmMulticlassTrainer and in LightGbmRankingTrainer.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.BatchSize">
            <summary>
            Number of entries in a batch when loading data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.UseCat">
            <summary>
            Enable categorical split or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.UseMissing">
            <summary>
            Enable missing value auto infer or not.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.MinDataPerGroup">
            <summary>
            Min number of instances per categorical group.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.MaxCatThreshold">
            <summary>
            Max number of categorical thresholds.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.CatSmooth">
            <summary>
            Lapalace smooth term in categorical feature spilt. Avoid the bias of small categories.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.CatL2">
            <summary>
            L2 Regularization for categorical split.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.ParallelTrainer">
            <summary>
            Parallel LightGBM Learning Algorithm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.GroupIdColumn">
            <summary>
            Column to use for example groupId
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LightGbmRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier">
            <summary>
            Train a linear SVM.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.Lambda">
            <summary>
            Regularizer constant
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.BatchSize">
            <summary>
            Batch size
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.PerformProjection">
            <summary>
            Perform projection to unit-ball? Typically used with batch size > 1.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.NoBias">
            <summary>
            No bias
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.Calibrator">
            <summary>
            The calibrator kind to apply to the predictor. Specify null for no calibration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.MaxCalibrationExamples">
            <summary>
            The maximum number of examples to use when training the calibrator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.NumIterations">
            <summary>
            Number of iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.InitialWeights">
            <summary>
            Initial Weights and bias, comma-separated
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.Shuffle">
            <summary>
            Whether to shuffle for each training iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.StreamingCacheSize">
            <summary>
            Size of cache when trained in Scope
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LinearSvmBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier">
            <summary>
        Logistic Regression is a method in statistics used to predict the probability of occurrence of an event and can be used as 
        a classification algorithm. The algorithm predicts the probability of occurrence of an event by fitting data to a logistical function.
      </summary><remarks>
        If the dependent variable has more than two possible values (blood type given diagnostic test results), 
        then the logistic regression is multinomial.
        <para>
          The optimization technique used for LogisticRegression Classifier is based on the limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).
          Both the L-BFGS and regular BFGS algorithms use quasi-Newtonian methods to estimate the computationally intensive 
          Hessian matrix in the equation used by Newton's method to calculate steps.
          But the L-BFGS approximation uses only a limited amount of memory to compute the next step direction,
          so that it is especially suited for problems with a large number of variables.
          The MemorySize argument specifies the number of past positions and gradients to store for use in the 
          computation of the next step.
        </para>
        <para>
          This learner can use elastic net regularization: a linear combination of L1 (LASSO) and L2 (ridge) regularizations.
          Regularization is a method that can render an ill-posed problem more tractable by imposing constraints that provide information 
          to supplement the data and that prevents overfitting by penalizing models with extreme coefficient values.
          This can improve the generalization of the model learned by selecting the optimal complexity in the bias-variance tradeoff.
          Regularization works by adding the penalty that is associated with coefficient values to the error of the hypothesis.
          An accurate model with extreme coefficient values would be penalized more, but a less accurate model with more conservative 
          values would be penalized less. L1 and L2 regularization have different effects and uses that are complementary in certain respects.
        </para>
          <list type="bullet">
            <item><description>
              L1Weight can be applied to sparse models, when working with high-dimensional data. It pulls small weights associated features
              that are relatively unimportant towards 0.
              L1 regularization is an implementation of OWLQN, based on:
              <a href="https://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.5260">Scalable training of L1-regularized log-linear models</a>
            </description></item>
            <item><description>L2Weight is preferable for data that is not sparse. It pulls large weights towards zero.</description></item>
          </list>
          Adding the ridge penalty to the regularization overcomes some of lasso's limitations. It can improve its predictive accuracy, for example, when the number of predictors is greater than the sample size. If x = l1_weight and y = l2_weight, ax + by = c defines the linear span of the regularization terms.
          The default values of x and y are both 1.
          An agressive regularization can harm predictive capacity by excluding important variables out of the model. So choosing the optimal values for the regularization parameters is important for the performance of the logistic regression model.
        <para>For more information see:</para>
        <list type="bullet">
          <item><description><a href="https://research.microsoft.com/apps/pubs/default.aspx?id=78900">Scalable Training of L1-Regularized Log-Linear Models</a>.</description></item>
          <item><description><a href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx">Test Run - L1 and L2 Regularization for Machine Learning</a>.</description></item>
          <item><description><a href="https://en.wikipedia.org/wiki/L-BFGS">Wikipedia: L-BFGS</a>.</description></item>
          <item><description><a href="https://en.wikipedia.org/wiki/Logistic_regression">Wikipedia: Logistic regression</a>.</description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new LogisticRegressionBinaryClassifier());
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.ShowTrainingStats">
            <summary>
            Show statistics of training examples.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.L2Weight">
            <summary>
            L2 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.L1Weight">
            <summary>
            L1 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.OptTol">
            <summary>
            Tolerance parameter for optimization convergence. Lower = slower, more accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.MemorySize">
            <summary>
            Memory size for L-BFGS. Lower=faster, less accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.MaxIterations">
            <summary>
            Maximum iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.SgdInitializationTolerance">
            <summary>
            Run SGD to initialize LR weights, converging to this tolerance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.Quiet">
            <summary>
            If set to true, produce no output during training.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.UseThreads">
            <summary>
            Whether or not to use threads. Default is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.NumThreads">
            <summary>
            Number of threads
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.DenseOptimizer">
            <summary>
            Force densification of the internal optimization vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.EnforceNonNegativity">
            <summary>
            Enforce non-negative weights
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier">
            <summary>
        Logistic Regression is a method in statistics used to predict the probability of occurrence of an event and can be used as 
        a classification algorithm. The algorithm predicts the probability of occurrence of an event by fitting data to a logistical function.
      </summary><remarks>
        If the dependent variable has more than two possible values (blood type given diagnostic test results), 
        then the logistic regression is multinomial.
        <para>
          The optimization technique used for LogisticRegression Classifier is based on the limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).
          Both the L-BFGS and regular BFGS algorithms use quasi-Newtonian methods to estimate the computationally intensive 
          Hessian matrix in the equation used by Newton's method to calculate steps.
          But the L-BFGS approximation uses only a limited amount of memory to compute the next step direction,
          so that it is especially suited for problems with a large number of variables.
          The MemorySize argument specifies the number of past positions and gradients to store for use in the 
          computation of the next step.
        </para>
        <para>
          This learner can use elastic net regularization: a linear combination of L1 (LASSO) and L2 (ridge) regularizations.
          Regularization is a method that can render an ill-posed problem more tractable by imposing constraints that provide information 
          to supplement the data and that prevents overfitting by penalizing models with extreme coefficient values.
          This can improve the generalization of the model learned by selecting the optimal complexity in the bias-variance tradeoff.
          Regularization works by adding the penalty that is associated with coefficient values to the error of the hypothesis.
          An accurate model with extreme coefficient values would be penalized more, but a less accurate model with more conservative 
          values would be penalized less. L1 and L2 regularization have different effects and uses that are complementary in certain respects.
        </para>
          <list type="bullet">
            <item><description>
              L1Weight can be applied to sparse models, when working with high-dimensional data. It pulls small weights associated features
              that are relatively unimportant towards 0.
              L1 regularization is an implementation of OWLQN, based on:
              <a href="https://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.5260">Scalable training of L1-regularized log-linear models</a>
            </description></item>
            <item><description>L2Weight is preferable for data that is not sparse. It pulls large weights towards zero.</description></item>
          </list>
          Adding the ridge penalty to the regularization overcomes some of lasso's limitations. It can improve its predictive accuracy, for example, when the number of predictors is greater than the sample size. If x = l1_weight and y = l2_weight, ax + by = c defines the linear span of the regularization terms.
          The default values of x and y are both 1.
          An agressive regularization can harm predictive capacity by excluding important variables out of the model. So choosing the optimal values for the regularization parameters is important for the performance of the logistic regression model.
        <para>For more information see:</para>
        <list type="bullet">
          <item><description><a href="https://research.microsoft.com/apps/pubs/default.aspx?id=78900">Scalable Training of L1-Regularized Log-Linear Models</a>.</description></item>
          <item><description><a href="https://msdn.microsoft.com/en-us/magazine/dn904675.aspx">Test Run - L1 and L2 Regularization for Machine Learning</a>.</description></item>
          <item><description><a href="https://en.wikipedia.org/wiki/L-BFGS">Wikipedia: L-BFGS</a>.</description></item>
          <item><description><a href="https://en.wikipedia.org/wiki/Logistic_regression">Wikipedia: Logistic regression</a>.</description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new LogisticRegressionClassifier());
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.ShowTrainingStats">
            <summary>
            Show statistics of training examples.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.L2Weight">
            <summary>
            L2 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.L1Weight">
            <summary>
            L1 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.OptTol">
            <summary>
            Tolerance parameter for optimization convergence. Lower = slower, more accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.MemorySize">
            <summary>
            Memory size for L-BFGS. Lower=faster, less accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.MaxIterations">
            <summary>
            Maximum iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.SgdInitializationTolerance">
            <summary>
            Run SGD to initialize LR weights, converging to this tolerance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.Quiet">
            <summary>
            If set to true, produce no output during training.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.UseThreads">
            <summary>
            Whether or not to use threads. Default is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.NumThreads">
            <summary>
            Number of threads
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.DenseOptimizer">
            <summary>
            Force densification of the internal optimization vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.EnforceNonNegativity">
            <summary>
            Enforce non-negative weights
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier">
            <summary>
        Trains a multiclass Naive Bayes predictor that supports binary feature values.
      </summary><remarks>
        <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> is a probabilistic classifier that can be used for multiclass problems.
        Using Bayes' theorem, the conditional probability for a sample belonging to a class can be calculated based on the sample count for each feature combination groups.
        However, Naive Bayes Classifier is feasible only if the number of features and the values each feature can take is relatively small.
        It assumes independence among the presence of features in a class even though they may be dependent on each other.
        This multi-class trainer accepts binary feature values of type float, i.e., feature values are either true or false.
        Specifically a feature value greater than zero is treated as true.
      </remarks><seealso cref="T:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Trainers.LightGbmClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier" /><seealso cref="T:Microsoft.ML.Legacy.Models.OneVersusAll" />
            <example>
        <code language="csharp">
          pipeline.Add(new NaiveBayesClassifier
            { 
              NormalizeFeatures = NormalizeOption.Auto,
              Caching = CachingOptions.Memory 
            });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.NaiveBayesClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor">
            <summary>
        Stochastic gradient descent is an optimization method used to train a wide range of models in machine learning. 
        In the ML.Net the implementation of OGD, is for linear regression. 
      </summary><remarks>
        Stochastic gradient descent uses a simple yet efficient iterative technique to fit model coefficients using error gradients for convex loss functions.
        The OnlineGradientDescentRegressor implements the standard (non-batch) SGD, with a choice of loss functions,
        and an option to update the weight vector using the average of the vectors seen over time (averaged argument is set to True by default).
      </remarks>
            <example>
        <code language="csharp">
          new OnlineGradientDescentRegressor
          {
            NumIterations = 10,
            L2RegularizerWeight = 0.6f,
            LossFunction = new PoissonLossRegressionLossFunction()
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.LearningRate">
            <summary>
            Learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.DecreaseLearningRate">
            <summary>
            Decrease learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.ResetWeightsAfterXExamples">
            <summary>
            Number of examples after which weights will be reset to the current average
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.DoLazyUpdates">
            <summary>
            Instead of updating averaged weights on every example, only update when loss is nonzero
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.L2RegularizerWeight">
            <summary>
            L2 Regularization Weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.RecencyGain">
            <summary>
            Extra weight given to more recent updates
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.RecencyGainMulti">
            <summary>
            Whether Recency Gain is multiplicative (vs. additive)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.Averaged">
            <summary>
            Do averaging?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.AveragedTolerance">
            <summary>
            The inexactness tolerance for averaging
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.NumIterations">
            <summary>
            Number of iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.InitialWeights">
            <summary>
            Initial Weights and bias, comma-separated
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.Shuffle">
            <summary>
            Whether to shuffle for each training iteration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.StreamingCacheSize">
            <summary>
            Size of cache when trained in Scope
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OnlineGradientDescentRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor">
            <summary>
        Train an OLS regression model.
      </summary><remarks>
        <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary least squares (OLS)</a> is a parameterized regression method. 
        It assumes that the conditional mean of the dependent variable follows a linear function of the dependent variables.
        The parameters of the regressor can be estimated by minimizing the squares of the difference between observed values and the predictions.
      </remarks><example>
        <code language="csharp">
          new OrdinaryLeastSquaresRegressor
          {
            L2Weight = 0.1,
            PerParameterSignificance = false,
            NormalizeFeatures  = Microsoft.ML.Models.NormalizeOption.Yes
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.L2Weight">
            <summary>
            L2 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.PerParameterSignificance">
            <summary>
            Whether to calculate per parameter significance statistics
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.OrdinaryLeastSquaresRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
            <example>
        <code language="csharp">
          new PcaAnomalyDetector
          {
            Rank = 40,
            Oversampling = 40,
            NormalizeFeatures = Microsoft.ML.Models.NormalizeOption.Warn
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Rank">
            <summary>
            The number of components in the PCA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Oversampling">
            <summary>
            Oversampling parameter for randomized PCA training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Center">
            <summary>
            If enabled, data is centered to be zero mean
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Seed">
            <summary>
            The seed for random number generation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PcaAnomalyDetector.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.PoissonRegressor">
            <summary>
        Trains a Poisson Regression model.  
      </summary><remarks>
        <a href="https://en.wikipedia.org/wiki/Poisson_regression">Poisson regression</a> is a parameterized regression method.
        It assumes that the log of the conditional mean of the dependent variable follows a linear function of the dependent variables.
        Assuming that the dependent variable follows a Poisson distribution, the parameters of the regressor can be estimated by maximizing the likelihood of the obtained observations.
      </remarks>
            <example>
        <code language="csharp">
          new PoissonRegressor
          {
            MaxIterations = 100,
            L2Weight = 0.6f
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.L2Weight">
            <summary>
            L2 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.L1Weight">
            <summary>
            L1 regularization weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.OptTol">
            <summary>
            Tolerance parameter for optimization convergence. Lower = slower, more accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.MemorySize">
            <summary>
            Memory size for L-BFGS. Lower=faster, less accurate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.MaxIterations">
            <summary>
            Maximum iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.SgdInitializationTolerance">
            <summary>
            Run SGD to initialize LR weights, converging to this tolerance
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.Quiet">
            <summary>
            If set to true, produce no output during training.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.InitWtsDiameter">
            <summary>
            Init weights diameter
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.UseThreads">
            <summary>
            Whether or not to use threads. Default is true
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.NumThreads">
            <summary>
            Number of threads
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.DenseOptimizer">
            <summary>
            Force densification of the internal optimization vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.EnforceNonNegativity">
            <summary>
            Enforce non-negative weights
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.PoissonRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier">
            <summary>
        Train an SDCA linear model.
      </summary><remarks>
        This classifier is a trainer based on the Stochastic Dual Coordinate Ascent(SDCA) method, a state-of-the-art optimization technique for convex objective functions.
        The algorithm can be scaled for use on large out-of-memory data sets due to a semi-asynchronized implementation that supports multi-threading.
        <para>
          Convergence is underwritten by periodically enforcing synchronization between primal and dual updates in a separate thread.
          Several choices of loss functions are also provided.
          The SDCA method combines several of the best properties and capabilities of logistic regression and SVM algorithms.
        </para>
        <para>
          Note that SDCA is a stochastic and streaming optimization algorithm.
          The results depends on the order of the training data. For reproducible results, it is recommended that one sets 'Shuffle' to
          False and 'NumThreads' to 1.
          Elastic net regularization can be specified by the 'L2Const' and 'L1Threshold' parameters. Note that the 'L2Const' has an effect on the rate of convergence.
          In general, the larger the 'L2Const', the faster SDCA converges.
        </para>
        <para>For more information, see:</para>
        <list type="bullet">
          <item><description>
            <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/main-3.pdf">Scaling Up Stochastic Dual Coordinate Ascent</a>.
          </description></item>
          <item><description>
            <a href="http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf">Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</a>.
          </description></item>
        </list>
       </remarks>
            <example>
        <code language="csharp">
          new StochasticDualCoordinateAscentBinaryClassifier
          {
            MaxIterations = 100,
            NumThreads = 7,
            LossFunction = new SmoothedHingeLossSDCAClassificationLossFunction(),
            Caching = Microsoft.ML.Models.CachingOptions.Disk
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.PositiveInstanceWeight">
            <summary>
            Apply weight to the positive class, for imbalanced data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.Calibrator">
            <summary>
            The calibrator kind to apply to the predictor. Specify null for no calibration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.MaxCalibrationExamples">
            <summary>
            The maximum number of examples to use when training the calibrator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.L2Const">
            <summary>
            L2 regularizer constant. By default the l2 constant is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.L1Threshold">
            <summary>
            L1 soft threshold (L1/L2). Note that it is easier to control and sweep using the threshold parameter than the raw L1-regularizer constant. By default the l1 threshold is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.NumThreads">
            <summary>
            Degree of lock-free parallelism. Defaults to automatic. Determinism not guaranteed.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.ConvergenceTolerance">
            <summary>
            The tolerance for the ratio between duality gap and primal loss for convergence checking.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.MaxIterations">
            <summary>
            Maximum number of iterations; set to 1 to simulate online learning. Defaults to automatic.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.Shuffle">
            <summary>
            Shuffle data every epoch?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.CheckFrequency">
            <summary>
            Convergence check frequency (in terms of number of iterations). Set as negative or zero for not checking at all. If left blank, it defaults to check after every 'numThreads' iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.BiasLearningRate">
            <summary>
            The learning rate for adjusting bias from being regularized.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier">
            <summary>
        Train an SDCA linear model.
      </summary><remarks>
        This classifier is a trainer based on the Stochastic Dual Coordinate Ascent(SDCA) method, a state-of-the-art optimization technique for convex objective functions.
        The algorithm can be scaled for use on large out-of-memory data sets due to a semi-asynchronized implementation that supports multi-threading.
        <para>
          Convergence is underwritten by periodically enforcing synchronization between primal and dual updates in a separate thread.
          Several choices of loss functions are also provided.
          The SDCA method combines several of the best properties and capabilities of logistic regression and SVM algorithms.
        </para>
        <para>
          Note that SDCA is a stochastic and streaming optimization algorithm.
          The results depends on the order of the training data. For reproducible results, it is recommended that one sets 'Shuffle' to
          False and 'NumThreads' to 1.
          Elastic net regularization can be specified by the 'L2Const' and 'L1Threshold' parameters. Note that the 'L2Const' has an effect on the rate of convergence.
          In general, the larger the 'L2Const', the faster SDCA converges.
        </para>
        <para>For more information, see:</para>
        <list type="bullet">
          <item><description>
            <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/main-3.pdf">Scaling Up Stochastic Dual Coordinate Ascent</a>.
          </description></item>
          <item><description>
            <a href="http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf">Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</a>.
          </description></item>
        </list>
       </remarks>
            <example>
        <code language="csharp">
          new StochasticDualCoordinateAscentClassifier
          {
            MaxIterations = 100,
            NumThreads = 7,
            LossFunction = new SmoothedHingeLossSDCAClassificationLossFunction()
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.L2Const">
            <summary>
            L2 regularizer constant. By default the l2 constant is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.L1Threshold">
            <summary>
            L1 soft threshold (L1/L2). Note that it is easier to control and sweep using the threshold parameter than the raw L1-regularizer constant. By default the l1 threshold is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.NumThreads">
            <summary>
            Degree of lock-free parallelism. Defaults to automatic. Determinism not guaranteed.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.ConvergenceTolerance">
            <summary>
            The tolerance for the ratio between duality gap and primal loss for convergence checking.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.MaxIterations">
            <summary>
            Maximum number of iterations; set to 1 to simulate online learning. Defaults to automatic.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.Shuffle">
            <summary>
            Shuffle data every epoch?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.CheckFrequency">
            <summary>
            Convergence check frequency (in terms of number of iterations). Set as negative or zero for not checking at all. If left blank, it defaults to check after every 'numThreads' iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.BiasLearningRate">
            <summary>
            The learning rate for adjusting bias from being regularized.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor">
            <summary>
        Train an SDCA linear model.
      </summary><remarks>
        This classifier is a trainer based on the Stochastic Dual Coordinate Ascent(SDCA) method, a state-of-the-art optimization technique for convex objective functions.
        The algorithm can be scaled for use on large out-of-memory data sets due to a semi-asynchronized implementation that supports multi-threading.
        <para>
          Convergence is underwritten by periodically enforcing synchronization between primal and dual updates in a separate thread.
          Several choices of loss functions are also provided.
          The SDCA method combines several of the best properties and capabilities of logistic regression and SVM algorithms.
        </para>
        <para>
          Note that SDCA is a stochastic and streaming optimization algorithm.
          The results depends on the order of the training data. For reproducible results, it is recommended that one sets 'Shuffle' to
          False and 'NumThreads' to 1.
          Elastic net regularization can be specified by the 'L2Const' and 'L1Threshold' parameters. Note that the 'L2Const' has an effect on the rate of convergence.
          In general, the larger the 'L2Const', the faster SDCA converges.
        </para>
        <para>For more information, see:</para>
        <list type="bullet">
          <item><description>
            <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/main-3.pdf">Scaling Up Stochastic Dual Coordinate Ascent</a>.
          </description></item>
          <item><description>
            <a href="http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf">Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</a>.
          </description></item>
        </list>
       </remarks>
            <example>
        <code language="csharp">
          new StochasticDualCoordinateAscentRegressor
          {
            MaxIterations = 100,
            NumThreads = 5
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.L2Const">
            <summary>
            L2 regularizer constant. By default the l2 constant is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.L1Threshold">
            <summary>
            L1 soft threshold (L1/L2). Note that it is easier to control and sweep using the threshold parameter than the raw L1-regularizer constant. By default the l1 threshold is automatically inferred based on data set.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.NumThreads">
            <summary>
            Degree of lock-free parallelism. Defaults to automatic. Determinism not guaranteed.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.ConvergenceTolerance">
            <summary>
            The tolerance for the ratio between duality gap and primal loss for convergence checking.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.MaxIterations">
            <summary>
            Maximum number of iterations; set to 1 to simulate online learning. Defaults to automatic.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.Shuffle">
            <summary>
            Shuffle data every epoch?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.CheckFrequency">
            <summary>
            Convergence check frequency (in terms of number of iterations). Set as negative or zero for not checking at all. If left blank, it defaults to check after every 'numThreads' iterations.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.BiasLearningRate">
            <summary>
            The learning rate for adjusting bias from being regularized.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentRegressor.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier">
            <summary>
            Train an Hogwild SGD binary model.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.LossFunction">
            <summary>
            Loss Function
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.L2Const">
            <summary>
            L2 regularizer constant
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.NumThreads">
            <summary>
            Degree of lock-free parallelism. Defaults to automatic depending on data sparseness. Determinism not guaranteed.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.ConvergenceTolerance">
            <summary>
            Exponential moving averaged improvement tolerance for convergence
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.MaxIterations">
            <summary>
            Maximum number of iterations; set to 1 to simulate online learning.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.InitLearningRate">
            <summary>
            Initial learning rate (only used by SGD)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.Shuffle">
            <summary>
            Shuffle data every epoch?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.PositiveInstanceWeight">
            <summary>
            Apply weight to the positive class, for imbalanced data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.CheckFrequency">
            <summary>
            Convergence check frequency (in terms of number of iterations). Default equals number of threads
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.Calibrator">
            <summary>
            The calibrator kind to apply to the predictor. Specify null for no calibration
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.MaxCalibrationExamples">
            <summary>
            The maximum number of examples to use when training the calibrator
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.WeightColumn">
            <summary>
            Column to use for example weight
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.StochasticGradientDescentBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier">
            <summary>
        Parallel Stochastic Gradient Descent trainer.
      </summary><remarks>
        <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient descent (SGD)</a> is an interative algorithm
        that optimizes a differentiable objective function. <a href="https://arxiv.org/abs/1705.08030">SYMSGD</a> parallelizes SGD using Sound Combiners.
      </remarks><example>
        <code language="csharp">
          new SymSgdBinaryClassifier()
          {
            NumberOfIterations = 50,
            L2Regularization = 0,
            Shuffle = true
          }
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.NumberOfThreads">
            <summary>
            Degree of lock-free parallelism. Determinism not guaranteed. Multi-threading is not supported currently.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.NumberOfIterations">
            <summary>
            Number of passes over the data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.Tolerance">
            <summary>
            Tolerance for difference in average loss in consecutive passes.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.LearningRate">
            <summary>
            Learning rate
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.L2Regularization">
            <summary>
            L2 regularization
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.UpdateFrequency">
            <summary>
            The number of iterations each thread learns a local model until combining it with the global model. Low value means more updated global model and high value means less cache traffic.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.MemorySize">
            <summary>
            The acceleration memory budget in MB
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.Shuffle">
            <summary>
            Shuffle data?
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.PositiveInstanceWeight">
            <summary>
            Apply weight to the positive class, for imbalanced data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.TrainingData">
            <summary>
            The data to be used for training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.FeatureColumn">
            <summary>
            Column to use for features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.NormalizeFeatures">
            <summary>
            Normalize option for the feature column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.Caching">
            <summary>
            Whether learner should cache input training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Trainers.SymSgdBinaryClassifier.Output.PredictorModel">
            <summary>
            The trained model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler">
            <summary>
            Approximate bootstrap sampling.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.Complement">
            <summary>
            Whether this is the out-of-bag sample, that is, all those rows that are not selected by the transform.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.Seed">
            <summary>
            The random seed. If unspecified random state will be instead derived from the environment.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.ShuffleInput">
            <summary>
            Whether we should attempt to shuffle the source data. By default on, but can be turned off for efficiency.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.PoolSize">
            <summary>
            When shuffling the output, the number of output rows to keep in that pool. Note that shuffling of output is completely distinct from shuffling of input.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ApproximateBootstrapSampler.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.BinaryPredictionScoreColumnsRenamer">
            <summary>
            For binary prediction, it renames the PredictedLabel and Score columns to include the name of the positive class.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinaryPredictionScoreColumnsRenamer.PredictorModel">
            <summary>
            The predictor model used in scoring
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinaryPredictionScoreColumnsRenamer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinaryPredictionScoreColumnsRenamer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinaryPredictionScoreColumnsRenamer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformBinColumn.NumBins">
            <summary>
            Max number of bins, power of 2 recommended
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformBinColumn.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformBinColumn.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformBinColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformBinColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.BinNormalizer">
            <summary>
            The values are assigned into equidensity bins and a value is mapped to its bin_number/number_of_bins.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.NumBins">
            <summary>
            Max number of bins, power of 2 recommended
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.BinNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.HashBits">
            <summary>
            The number of bits to hash into. Must be between 1 and 30, inclusive.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.Seed">
            <summary>
            Hashing seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.Ordered">
            <summary>
            Whether the position of each term should be included in the hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.InvertHash">
            <summary>
            Limit the number of keys used to generate the slot name to this many. 0 means no invert hashing, -1 means no limit.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.OutputKind">
            <summary>
            Output kind: Bag (multi-set vector), Ind (indicator vector), or Key (index)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer">
            <summary>
        Encodes the categorical variable with hash-based encoding. 
      </summary><remarks>
        CategoricalHashOneHotVectorizer converts a categorical value into an indicator array by hashing the value and using the hash as an index in the bag.
        If the input column is a vector, a single indicator bag is returned for it.
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new CategoricalHashOneHotVectorizer("Text1") 
          { 
            HashBits = 10, 
            Seed = 314489979, 
            OutputKind = CategoricalTransformOutputKind.Bag 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Column">
            <summary>
            New column definition(s) (optional form: name:hashBits:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.HashBits">
            <summary>
            Number of bits to hash into. Must be between 1 and 30, inclusive.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Seed">
            <summary>
            Hashing seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Ordered">
            <summary>
            Whether the position of each term should be included in the hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.InvertHash">
            <summary>
            Limit the number of keys used to generate the slot name to this many. 0 means no invert hashing, -1 means no limit.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.OutputKind">
            <summary>
            Output kind: Bag (multi-set vector), Ind (indicator vector), or Key (index)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalHashOneHotVectorizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.OutputKind">
            <summary>
            Output kind: Bag (multi-set vector), Ind (indicator vector), Key (index), or Binary encoded indicator vector
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.MaxNumTerms">
            <summary>
            Maximum number of terms to keep when auto-training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.TextKeyValues">
            <summary>
            Whether key value metadata should be text, regardless of the actual input type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer">
            <summary>
        Converts the categorical value into an indicator array by building a dictionary of categories based on the data and using the id in the dictionary as the index in the array
      </summary><remarks>
        <para>
        The CategoricalOneHotVectorizer transform passes through a data set, operating on text columns, to
        build a dictionary of categories.
        For each row, the entire text string appearing in the input column is defined as a category.</para>
        <para>The output of this transform is an indicator vector.</para>
        Each slot in this vector corresponds to a category in the dictionary, so its length is the size of the built dictionary.
        <para>The CategoricalOneHotVectorizer can be applied to one or more columns, in which case it builds and uses a separate dictionary
        for each column that it is applied to.</para>
        
        <para>The <see cref="T:Microsoft.ML.Transforms.CategoricalTransformOutputKind" /> produces integer values and KeyType columns.
        The Key value is the one-based index of the slot set in the Ind/Bag options.
        If the Key option is not found, it is assigned the value zero.
        In the <see cref="F:Microsoft.ML.Transforms.CategoricalTransformOutputKind.Ind" />, <see cref="F:Microsoft.ML.Transforms.CategoricalTransformOutputKind.Bag" /> options are not found, they result in an all zero bit vector.
        <see cref="F:Microsoft.ML.Transforms.CategoricalTransformOutputKind.Ind" /> and <see cref="F:Microsoft.ML.Transforms.CategoricalTransformOutputKind.Bag" /> differ simply in how the bit-vectors generated from individual slots are aggregated:
        for Ind they are concatenated and for Bag they are added.
        When the source column is a singleton, the Ind and Bag options are identical.</para>
      </remarks>
            <example>
        An example of how to add the CategoricalOneHotVectorizer transform to a pipeline with two text column
        features named "Text1" and "Text2".
        <code language="csharp">
          pipeline.Add(new CategoricalOneHotVectorizer("Text1", "Text1"));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.OutputKind">
            <summary>
            Output kind: Bag (multi-set vector), Ind (indicator vector), or Key (index)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.MaxNumTerms">
            <summary>
            Maximum number of terms to keep per column when auto-training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.TextKeyValues">
            <summary>
            Whether key value metadata should be text, regardless of the actual input type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharTokenizeTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharTokenizeTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.CharacterTokenizer">
            <summary>
        This transform breaks text into individual tokens, each consisting of an individual character.
      </summary><remarks>
      This transform is not typically used on its own, but it is one of the transforms composing the 
      <see cref="!:Microsoft.ML.Transforms.TextFeaturizer">Text Featurizer</see>. 
      </remarks><seealso cref="T:Microsoft.ML.Transforms.WordTokenizer" /><seealso cref="!:Microsoft.ML.Transforms.TextToKey" /><seealso cref="!:Microsoft.ML.Transforms.NGramTranslator" /><seealso cref="!:Microsoft.ML.Transforms.TextFeaturizer" /><example>
        <code language="csharp">
          pipeline.Add(new CharacterTokenizer("TextCol1" , "TextCol2" ));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharacterTokenizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharacterTokenizer.UseMarkerChars">
            <summary>
            Whether to mark the beginning/end of each row/slot with start of text character (0x02)/end of text character (0x03)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharacterTokenizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharacterTokenizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CharacterTokenizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConcatTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConcatTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ColumnConcatenator">
            <summary>
            Concatenates one or more columns of the same item type.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnConcatenator.Column">
            <summary>
            New column definition(s) (optional form: name:srcs)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnConcatenator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnConcatenator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnConcatenator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CopyColumnsTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CopyColumnsTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ColumnCopier">
            <summary>
            Duplicates columns from the dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnCopier.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnCopier.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnCopier.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnCopier.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ColumnDropper">
            <summary>
            Drops columns from the dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnDropper.Column">
            <summary>
            Column name to drop
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnDropper.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnDropper.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnDropper.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ColumnSelector">
            <summary>
            Selects a set of columns, dropping all others
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnSelector.Column">
            <summary>
            Column name to keep
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnSelector.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnSelector.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnSelector.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConvertTransformColumn.ResultType">
            <summary>
            The result type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConvertTransformColumn.Range">
            <summary>
            For a key column, this defines the range of values
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConvertTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConvertTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter">
            <summary>
            Converts a column to a different type, using standard conversions.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.Column">
            <summary>
            New column definition(s) (optional form: name:type:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.ResultType">
            <summary>
            The result type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.Range">
            <summary>
            For a key column, this defines the range of values
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ColumnTypeConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId">
            <summary>
       Groups values of a scalar column into a vector, by a contiguous group ID.
      </summary><remarks>
       The CombinerByContiguousGroupId transform groups the consecutive rows that share the specified group key (or keys). 
       Both group keys and the aggregated values can be of arbitrary non-vector types. 
       The resulting data will have all the group key columns preserved, 
       and the aggregated columns will become variable-length vectors of the original types.
       <para>This transform essentially performs the following SQL-like operation:</para> 
       <para>SELECT GroupKey1, GroupKey2, ... GroupKeyK, LIST(Value1), LIST(Value2), ... LIST(ValueN)</para> 
       <para>FROM Data</para> 
       <para>GROUP BY GroupKey1, GroupKey2, ... GroupKeyK.</para> 
      </remarks><seealso cref="!:Microsoft.ML.Transforms.Segregator" /><example>
        <code language="csharp">
          pipeline.Add(new CombinerByContiguousGroupId
          { 
              GroupKey = new []{"Key1", "Key2" } 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId.GroupKey">
            <summary>
            Columns to group by
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId.Column">
            <summary>
            Columns to group together
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.CombinerByContiguousGroupId.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformAffineColumn.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformAffineColumn.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformAffineColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformAffineColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer">
            <summary>
            Normalize the columns only if needed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ConditionalNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.DataCache">
            <summary>
            Caches using the specified cache option.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DataCache.Caching">
            <summary>
            Caching strategy
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DataCache.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DataCache.Output.OutputData">
            <summary>
            Dataset
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.DatasetScorer">
            <summary>
            Score a dataset with a predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetScorer.Data">
            <summary>
            The dataset to be scored
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetScorer.PredictorModel">
            <summary>
            The predictor model to apply to data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetScorer.Suffix">
            <summary>
            Suffix to append to the score columns
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetScorer.Output.ScoredData">
            <summary>
            The scored dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetScorer.Output.ScoringTransform">
            <summary>
            The scoring transform
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.DatasetTransformScorer">
            <summary>
            Score a dataset with a transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetTransformScorer.Data">
            <summary>
            The dataset to be scored
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetTransformScorer.TransformModel">
            <summary>
            The transform model to apply to data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetTransformScorer.Output.ScoredData">
            <summary>
            The scored dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DatasetTransformScorer.Output.ScoringTransform">
            <summary>
            The scoring transform
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.MaxNumTerms">
            <summary>
            Maximum number of terms to keep when auto-training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.TextKeyValues">
            <summary>
            Whether key value metadata should be text, regardless of the actual input type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.Dictionarizer">
            <summary>
            Converts input values (words, numbers, etc.) to index in a dictionary.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.MaxNumTerms">
            <summary>
            Maximum number of terms to keep per column when auto-training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.TextKeyValues">
            <summary>
            Whether key value metadata should be text, regardless of the actual input type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Dictionarizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.FeatureCombiner">
            <summary>
            Combines all the features into one feature column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureCombiner.Features">
            <summary>
            Features
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureCombiner.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureCombiner.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureCombiner.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount">
            <summary>
        Selects the slots for which the count of non-default values is greater than or equal to a threshold.
      </summary><remarks>
        <para>
          This transform uses a set of aggregators to count the number of non-default values for each slot and
          instantiates a <see cref="T:Microsoft.ML.Runtime.Data.DropSlotsTransform" /> to actually drop the slots.
          This transform is useful when applied together with a <see cref="T:Microsoft.ML.Transforms.CategoricalHashOneHotVectorizer" />. 
          The count feature selection can remove those features generated by the hash transform that have no data in the examples.
        </para>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new FeatureSelectorByCount
          { 
            Column = new[]{ "Feature1" }, 
            Count = 2 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount.Column">
            <summary>
            Columns to use for feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount.Count">
            <summary>
            If the count of non-default values for a slot is greater than or equal to this threshold, the slot is preserved
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByCount.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation">
            <summary>
        Selects the top k slots across all specified columns ordered by their mutual information with the label column.
      </summary><remarks>
        <para>
          The mutual information of two random variables X and Y is a measure of the mutual dependence between the variables.
          Formally, the mutual information can be written as:
        </para>
        <para>I(X;Y) = E[log(p(x,y)) - log(p(x)) - log(p(y))]</para>
        <para>where the expectation is taken over the joint distribution of X and Y. 
        Here p(x,y) is the joint probability density function of X and Y, p(x) and p(y) are the marginal probability density functions of X and Y respectively. 
        In general, a higher mutual information between the dependent variable (or label) and an independent variable (or feature) means 
        that the label has higher mutual dependence over that feature.
        It keeps the top SlotsInOutput features with the largest mutual information with the label.
        </para>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new FeatureSelectorByMutualInformation
          { 
            Column = new[]{ "Feature1" }, 
            SlotsInOutput = 6 
           });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.Column">
            <summary>
            Columns to use for feature selection
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.LabelColumn">
            <summary>
            Column to use for labels
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.SlotsInOutput">
            <summary>
            The maximum number of slots to preserve in output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.NumBins">
            <summary>
            Max number of bins for R4/R8 columns, power of 2 recommended
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.FeatureSelectorByMutualInformation.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformGcnColumn.UseStdDev">
            <summary>
            Normalize by standard deviation rather than L2 norm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformGcnColumn.Scale">
            <summary>
            Scale features by this value
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformGcnColumn.SubMean">
            <summary>
            Subtract mean from each value before normalizing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformGcnColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformGcnColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer">
            <summary>
        <para>Performs a global contrast normalization on input values:</para>
        <para>Y = (s * X - M) / D</para> 
        <para>where s is a scale, M is mean and D is either the L2 norm or standard deviation.</para>
       </summary><remarks>
        Scaling inputs to unit norms is a common operation for text classification or clustering.
        For more information see: 
        <a href="https://www.cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf">An Analysis of Single-Layer Networks in Unsupervised Feature Learning</a>
      </remarks><seealso cref="T:Microsoft.ML.Transforms.LpNormalizer" /><example>
        <code language="csharp">
          pipeline.Add(new GlobalContrastNormalizer("FeatureCol")
          { 
              SubMean= false
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.SubMean">
            <summary>
            Subtract mean from each value before normalizing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.UseStdDev">
            <summary>
            Normalize by standard deviation rather than L2 norm
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.Scale">
            <summary>
            Scale features by this value
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GlobalContrastNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.Join">
            <summary>
            Whether the values need to be combined for a single hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.CustomSlotMap">
            <summary>
            Which slots should be combined together. Example: 0,3,5;0,1;3;2,1,0. Overrides 'join'.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.HashBits">
            <summary>
            Number of bits to hash into. Must be between 1 and 31, inclusive.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.Seed">
            <summary>
            Hashing seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.Ordered">
            <summary>
            Whether the position of each term should be included in the hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashJoinTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.HashConverter">
            <summary>
        Converts multiple column values into hashes. 
        This transform accepts both numeric and text inputs, both single and vector-valued columns. 
      </summary><remarks>
        This transform can be helpful for ranking and cross-validation. In the case of ranking, where the GroupIdColumn column is required,
        and needs to be of a key type you can use the <see cref="T:Microsoft.ML.Transforms.CategoricalHashOneHotVectorizer" /> to hash the text value of a single GroupID column into a key value.
        If the GroupID is the combination of the values from multiple columns, you can use the HashConverter to hash multiple text columns into one key column. 
        Similarly with CrossValidator and the StratificationColumn. 
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new HashConverter("Column1", "Column2"));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Join">
            <summary>
            Whether the values need to be combined for a single hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.HashBits">
            <summary>
            Number of bits to hash into. Must be between 1 and 31, inclusive.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Seed">
            <summary>
            Hashing seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Ordered">
            <summary>
            Whether the position of each term should be included in the hash
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.HashConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscaleTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscaleTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ImageGrayscale">
            <summary>
            Convert image into grayscale.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscale.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscale.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscale.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageGrayscale.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoaderTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoaderTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ImageLoader">
            <summary>
            Load images from files.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoader.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoader.ImageFolder">
            <summary>
            Folder where to search for images
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoader.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoader.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageLoader.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.UseAlpha">
            <summary>
            Whether to use alpha channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.UseRed">
            <summary>
            Whether to use red channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.UseGreen">
            <summary>
            Whether to use green channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.UseBlue">
            <summary>
            Whether to use blue channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.InterleaveArgb">
            <summary>
            Whether to separate each channel or interleave in ARGB order
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.Convert">
            <summary>
            Whether to convert to floating point
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.Offset">
            <summary>
            Offset (pre-scale)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.Scale">
            <summary>
            Scale factor
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractorTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor">
            <summary>
            Extract color plane(s) from an image. Options include scaling, offset and conversion to floating point.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.UseAlpha">
            <summary>
            Whether to use alpha channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.UseRed">
            <summary>
            Whether to use red channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.UseGreen">
            <summary>
            Whether to use green channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.UseBlue">
            <summary>
            Whether to use blue channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.InterleaveArgb">
            <summary>
            Whether to separate each channel or interleave in ARGB order
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Convert">
            <summary>
            Whether to convert to floating point
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Offset">
            <summary>
            Offset (pre-scale)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Scale">
            <summary>
            Scale factor
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImagePixelExtractor.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.ImageWidth">
            <summary>
            Width of the resized image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.ImageHeight">
            <summary>
            Height of the resized image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.Resizing">
            <summary>
            Resizing method
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.CropAnchor">
            <summary>
            Anchor for cropping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizerTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ImageResizer">
            <summary>
            Scales an image to specified dimensions using one of the three scale types: isotropic with padding, isotropic with cropping or anisotropic. In case of isotropic padding, transparent color is used to pad resulting image.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.ImageWidth">
            <summary>
            Resized width of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.ImageHeight">
            <summary>
            Resized height of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.Resizing">
            <summary>
            Resizing method
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.CropAnchor">
            <summary>
            Anchor for cropping
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ImageResizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToValueTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToValueTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.KeyToTextConverter">
            <summary>
        Helps retrieving the original values from a key column. 
      </summary><remarks>
        The KeyToTextConverter is the complement of the <see cref="T:Microsoft.ML.Legacy.Transforms.TextToKeyConverter" /> transform.
        Since key values are an enumeration into the set of keys, most transforms that produce key valued outputs
        corresponding to input values will often, wherever possible, associate a piece of KeyValue metadata with that dataset.
        Transforming values into a categorical variable would be of limited use,
        if we couldn't somehow backtrack to figure out what those categories actually mean.
        The KeyToTextConverter enables that functionality.
      </remarks><seealso cref="!:Microsoft.ML.Transforms.HashConverter" /><seealso cref="!:Microsoft.ML.Transforms.TextToKeyConverter" /><example>
        <code language="csharp">
          pipeline.Add(new KeyToTextConverter(("InColumn", "OutColumn" )));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToTextConverter.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToTextConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToTextConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.KeyToTextConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter">
            <summary>
            Transforms the label to either key or bool (if needed) to make it suitable for classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter.TextKeyValues">
            <summary>
            Convert the key values to text
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter.LabelColumn">
            <summary>
            The label column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelColumnKeyBooleanConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicatorTransformColumn.ClassIndex">
            <summary>
            The positive example class for binary classification.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicatorTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicatorTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LabelIndicator">
            <summary>
            Label remapper used by OVA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicator.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicator.ClassIndex">
            <summary>
            Label of the positive class.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelIndicator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LabelToFloatConverter">
            <summary>
            Transforms the label to float to make it suitable for regression.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelToFloatConverter.LabelColumn">
            <summary>
            The label column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelToFloatConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelToFloatConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LabelToFloatConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumTopic">
            <summary>
            The number of topics in the LDA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.AlphaSum">
            <summary>
            Dirichlet prior on document-topic vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.Beta">
            <summary>
            Dirichlet prior on vocab-topic vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.Mhstep">
            <summary>
            Number of Metropolis Hasting step
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumIterations">
            <summary>
            Number of iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.LikelihoodInterval">
            <summary>
            Compute log likelihood over local dataset on this iteration interval
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumThreads">
            <summary>
            The number of training threads
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumMaxDocToken">
            <summary>
            The threshold of maximum count of tokens per doc
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumSummaryTermPerTopic">
            <summary>
            The number of words to summarize the topic
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.NumBurninIterations">
            <summary>
            The number of burn-in iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.ResetRandomGenerator">
            <summary>
            Reset the random number generator for each document
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LdaTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LightLda">
            <summary>
        The LDA transform implements LightLDA, a state-of-the-art implementation of Latent Dirichlet Allocation.
      </summary><remarks>
        Latent Dirichlet Allocation is a well-known topic modeling algorithm that infers topical structure from text data,
        and can be used to featurize any text fields as low-dimensional topical vectors. 
        <para>LightLDA is an extremely efficient implementation of LDA developed in MSR-Asia that incorporates a number of 
         optimization techniques. See <a href="https://arxiv.org/abs/1412.1576">LightLDA: Big Topic Models on Modest Compute Clusters</a>.
        </para>
        <para>
          With the LDA transform, ML.NET users can train a topic model to produce 1 million topics with 1 million vocabulary
          on a 1-billion-token document set one a single machine in a few hours (typically, LDA at this scale takes days and requires large clusters).
          The most significant innovation is a super-efficient O(1) <a href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm">Metropolis-Hastings sampling algorithm</a>,
          whose running cost is (surprisingly) agnostic of model size,
          allowing it to converges nearly an order of magnitude faster than other <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs samplers.</a>
        </para>
        <para>
          For more details please see original LightLDA paper, and its open source implementation. 
          <list type="bullet">
            <item><description><a href="https://arxiv.org/abs/1412.1576"> LightLDA: Big Topic Models on Modest Computer Clusters</a></description></item>
            <item><description><a href=" https://github.com/Microsoft/LightLDA">LightLDA </a></description></item>
          </list>
        </para>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new LightLda(("InTextCol" , "OutTextCol")));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Column">
            <summary>
            New column definition(s) (optional form: name:srcs)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumTopic">
            <summary>
            The number of topics in the LDA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.AlphaSum">
            <summary>
            Dirichlet prior on document-topic vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Beta">
            <summary>
            Dirichlet prior on vocab-topic vectors
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Mhstep">
            <summary>
            Number of Metropolis Hasting step
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumIterations">
            <summary>
            Number of iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.LikelihoodInterval">
            <summary>
            Compute log likelihood over local dataset on this iteration interval
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumMaxDocToken">
            <summary>
            The threshold of maximum count of tokens per doc
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumThreads">
            <summary>
            The number of training threads. Default value depends on number of logical processors.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumSummaryTermPerTopic">
            <summary>
            The number of words to summarize the topic
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.NumBurninIterations">
            <summary>
            The number of burn-in iterations
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.ResetRandomGenerator">
            <summary>
            Reset the random number generator for each document
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.OutputTopicWordSummary">
            <summary>
            Whether to output the topic-word summary in text format
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LightLda.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformLogNormalColumn.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformLogNormalColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NormalizeTransformLogNormalColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer">
            <summary>
            Normalizes the data based on the computed mean and variance of the logarithm of the data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.UseCdf">
            <summary>
            Whether to use CDF as the output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LogMeanVarianceNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformColumn.NormKind">
            <summary>
            The norm to use to normalize each sample
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformColumn.SubMean">
            <summary>
            Subtract mean from each value before normalizing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormNormalizerTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.LpNormalizer">
            <summary>
         The LpNormalizer transforms, normalizes vectors (rows) individually by rescaling them to unit norm (L2, L1 or LInf). 
         <para>Performs the following operation on a vector X:</para> 
         <para>Y = (X - M) / D</para> 
         <para>where M is mean and D is either L2 norm, L1 norm or LInf norm.</para>
       </summary><remarks>
        Scaling inputs to unit norms is a common operation for text classification or clustering.
        For more information see: <a href="https://www.cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf" />
      </remarks><seealso cref="T:Microsoft.ML.Transforms.GlobalContrastNormalizer" /><example>
        <code language="csharp">
          pipeline.Add(new LpNormalizer("FeatureCol")
          { 
              NormKind = LpNormNormalizerTransformNormalizerKind.L1Norm
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.NormKind">
            <summary>
            The norm to use to normalize each sample
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.SubMean">
            <summary>
            Subtract mean from each value before normalizing
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.LpNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ManyHeterogeneousModelCombiner">
            <summary>
            Combines a sequence of TransformModels and a PredictorModel into a single PredictorModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ManyHeterogeneousModelCombiner.TransformModels">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ManyHeterogeneousModelCombiner.PredictorModel">
            <summary>
            Predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ManyHeterogeneousModelCombiner.Output.PredictorModel">
            <summary>
            Predictor model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer">
            <summary>
            Normalizes the data based on the computed mean and variance of the data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.UseCdf">
            <summary>
            Whether to use CDF as the output
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MeanVarianceNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer">
            <summary>
            Normalizes the data based on the observed minimum and maximum values of the data.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.FixZero">
            <summary>
            Whether to map zero to zero, preserving sparsity
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.MaxTrainingExamples">
            <summary>
            Max number of examples used to train the normalizer
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAHandleTransformColumn.Kind">
            <summary>
            The replacement method to utilize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAHandleTransformColumn.ImputeBySlot">
            <summary>
            Whether to impute values by slot
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAHandleTransformColumn.ConcatIndicator">
            <summary>
            Whether or not to concatenate an indicator vector column to the value column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAHandleTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAHandleTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MissingValueHandler">
            <summary>
        Handle missing values by replacing them with either the default value or the indicated value.
      </summary><remarks>
        This transform handles missing values in the input columns. For each input column, it creates an output column
        where the missing values are replaced by one of these specified values:
        <list type="bullet">
          <item>
            <description>The default value of the appropriate type.</description>
          </item>
          <item>
            <description>The mean value of the appropriate type.</description>
          </item>
          <item>
            <description>The max value of the appropriate type.</description>
          </item>
          <item>
            <description>The min value of the appropriate type.</description>
          </item>
        </list>
        <para>The last three work only for numeric/TimeSpan/DateTime kind columns.</para>
        <para>
          The output column can also optionally include an indicator vector for which slots were missing in the input column.
          This can be done only when the indicator vector type can be converted to the input column type, i.e. only for numeric columns.
        </para>
        <para>
          When computing the mean/max/min value, there is also an option to compute it over the whole column instead of per slot.
          This option has a default value of true for variable length vectors, and false for known length vectors.
          It can be changed to true for known length vectors, but it results in an error if changed to false for variable length vectors.
        </para>
      </remarks><seealso cref="T:Microsoft.ML.Runtime.Data.MetadataUtils.Kinds.HasMissingValues" /><seealso cref="T:Microsoft.ML.Data.DataKind" />
            <example>
        <code language="csharp">
          pipeline.Add(new MissingValueHandler("FeatureCol", "CleanFeatureCol")
          {
              ReplaceWith  = NAHandleTransformReplacementKind.Mean
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.Column">
            <summary>
            New column definition(s) (optional form: name:rep:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.ReplaceWith">
            <summary>
            The replacement method to utilize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.ImputeBySlot">
            <summary>
            Whether to impute values by slot
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.Concat">
            <summary>
            Whether or not to concatenate an indicator vector column to the value column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueHandler.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAIndicatorTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAIndicatorTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MissingValueIndicator">
            <summary>
        This transform can transform either scalars or vectors (both fixed and variable size),
        creating output columns that indicate, through the true/false booleans whether the row has a missing value.
      </summary><seealso cref="T:Microsoft.ML.Runtime.Data.MetadataUtils.Kinds.HasMissingValues" />
            <example>
        <code language="csharp">
          pipeline.Add(new MissingValueIndicator("Column1"));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueIndicator.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueIndicator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueIndicator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueIndicator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NADropTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NADropTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MissingValuesDropper">
            <summary>
        Removes missing values from vector type columns.
      </summary><seealso cref="T:Microsoft.ML.Runtime.Data.MetadataUtils.Kinds.HasMissingValues" />
            <!-- No matching elements were found for the following include tag --><include file="../Microsoft.ML.Transforms/doc.xml" path="doc/members/example[@name=&quot;NADrop&quot;]/*" />
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesDropper.Column">
            <summary>
            Columns to drop the NAs for
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesDropper.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesDropper.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesDropper.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper">
            <summary>
        Removes missing values from vector type columns.
      </summary><remarks>
        This transform removes the entire row if any of the input columns have a missing value in that row.
        This preprocessing is required for many ML algorithms that cannot work with missing values.
        Useful if any missing entry invalidates the entire row.
        If the <see cref="P:Microsoft.ML.Transforms.MissingValuesRowDropper.Complement" /> is set to true, this transform would do the exact opposite,
        it will keep only the rows that have missing values.
      </remarks><seealso cref="T:Microsoft.ML.Runtime.Data.MetadataUtils.Kinds.HasMissingValues" />
            <example>
        <code language="csharp">
          pipeline.Add(new MissingValuesRowDropper("Column1"));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper.Column">
            <summary>
            Column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper.Complement">
            <summary>
            If true, keep only rows that contain NA values, and filter the rest.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValuesRowDropper.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAReplaceTransformColumn.ReplacementString">
            <summary>
            Replacement value for NAs (uses default value if not given)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAReplaceTransformColumn.Kind">
            <summary>
            The replacement method to utilize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAReplaceTransformColumn.Slot">
            <summary>
            Whether to impute values by slot
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAReplaceTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NAReplaceTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor">
            <summary>
        Create an output column of the same type and size of the input column, 
        where missing values are replaced with either the default value or the mean/min/max value (for non-text columns only). 
      </summary><remarks>
        This transform can transform either scalars or vectors (both fixed and variable size),
        creating output columns that are identical to the input columns except for replacing NA values
        with either the default value, user input, or imputed values (min/max/mean are currently supported).
        Imputation modes are supported for vectors both by slot and across all slots.
      </remarks><seealso cref="T:Microsoft.ML.Runtime.Data.MetadataUtils.Kinds.HasMissingValues" />
            <example>
        <code language="csharp">
          pipeline.Add(new MissingValueSubstitutor("FeatureCol")
          { 
              ReplacementKind = NAReplaceTransformReplacementKind.Mean 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.Column">
            <summary>
            New column definition(s) (optional form: name:rep:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.ReplacementKind">
            <summary>
            The replacement method to utilize
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.ImputeBySlot">
            <summary>
            Whether to impute values by slot
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.MissingValueSubstitutor.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ModelCombiner">
            <summary>
            Combines a sequence of TransformModels into a single model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ModelCombiner.Models">
            <summary>
            Input models
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ModelCombiner.Output.OutputModel">
            <summary>
            Combined model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.NgramLength">
            <summary>
            Maximum ngram length
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.AllLengths">
            <summary>
            Whether to include all ngram lengths up to NgramLength or only NgramLength
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.SkipLength">
            <summary>
            Maximum number of tokens to skip when constructing an ngram
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.MaxNumTerms">
            <summary>
            Maximum number of ngrams to store in the dictionary
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.Weighting">
            <summary>
            Statistical measure used to evaluate how important a word is to a document in a corpus
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NgramTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.NGramTranslator">
            <summary>
        This transform produces a bag of counts of n-grams (sequences of consecutive values of length 1-n) in a given vector of keys. 
        It does so by building a dictionary of n-grams and using the id in the dictionary as the index in the bag.
      </summary><remarks>
        This transform produces a matrix of token ngrams/skip-grams counts for a given corpus of text.
        The n-grams are represented as count vectors, with vector slots corresponding to n-grams.
        Embedding ngrams in a vector space allows their contents to be compared in an efficient manner. 
        The slot values in the vector can be weighted by the following factors:
        <list type="bullet">
          <item>
            <term>term frequency</term>
            <description> the number of occurrences of the slot in the text</description>
          </item>
          <item>
            <term>inverse document frequency</term>
            <description> a ratio (the logarithm of inverse relative slot frequency)
              that measures the information a slot provides by determining how common or rare it is across the entire text.</description>
          </item>
            <item>
              <term>term frequency-inverse document frequency</term>
              <description> the product term frequency and the inverse document frequency.</description>
            </item>
        </list>
        This transform is not typically used on its own, but it is one of the transforms composing the <see cref="!:Microsoft.ML.Transforms.TextFeaturizer">Text Featurizer</see> .
      </remarks><seealso cref="T:Microsoft.ML.Transforms.WordTokenizer" /><seealso cref="!:Microsoft.ML.Transforms.TextToKey" /><seealso cref="!:Microsoft.ML.Transforms.TextFeaturizer" /><seealso cref="T:Microsoft.ML.Transforms.CharacterTokenizer" /><example>
        <code language="csharp">
          pipeline.Add(new NGramTranslator("TextColumn")
          { 
            Weighting=NgramTransformWeightingCriteria.TfIdf  
          });
      </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.NgramLength">
            <summary>
            Maximum ngram length
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.AllLengths">
            <summary>
            Whether to store all ngram lengths up to ngramLength, or only ngramLength
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.SkipLength">
            <summary>
            Maximum number of tokens to skip when constructing an ngram
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.MaxNumTerms">
            <summary>
            Maximum number of ngrams to store in the dictionary
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.Weighting">
            <summary>
            The weighting criteria
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NGramTranslator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.NoOperation">
            <summary>
            Does nothing.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NoOperation.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NoOperation.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.NoOperation.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.OptionalColumnCreator">
            <summary>
        Creates a new column with the specified type and default values.
      </summary><remarks>
        If the user wish to create additional columns with a particular type and default values, or replicated the values from one column to another, changing their type, they can do so using this transform.
        This transform can be used as a workaround to create a Label column after deserializing a model, for prediction.
        Some transforms in the serialized model operate on the Label column, and would throw errors during prediction if such a column is not found.
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new OptionalColumnCreator 
          { 
            Column = new[]{ "OptColumn"} 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.OptionalColumnCreator.Column">
            <summary>
            New column definition(s)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.OptionalColumnCreator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.OptionalColumnCreator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.OptionalColumnCreator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.WeightColumn">
            <summary>
            The name of the weight column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Rank">
            <summary>
            The number of components in the PCA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Oversampling">
            <summary>
            Oversampling parameter for randomized PCA training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Center">
            <summary>
            If enabled, data is centered to be zero mean
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Seed">
            <summary>
            The seed for random number generation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.PcaCalculator">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
            <example>
        An example of how to add the PcaCalculator transform to a pipeline with a column named "Features".
        <code language="csharp">
          string[] features = new string["Sepal length", "Sepal width", "Petal length", "Petal width"];
          pipeline.Add(new PcaCalculator(columns){ Rank = 3 });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.WeightColumn">
            <summary>
            The name of the weight column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Rank">
            <summary>
            The number of components in the PCA
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Oversampling">
            <summary>
            Oversampling parameter for randomized PCA training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Center">
            <summary>
            If enabled, data is centered to be zero mean
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Seed">
            <summary>
            The seed for random number generation
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PcaCalculator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.PredictedLabelColumnOriginalValueConverter">
            <summary>
            Transforms a predicted label column to its original values, unless it is of type bool.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PredictedLabelColumnOriginalValueConverter.PredictedLabelColumn">
            <summary>
            The predicted label column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PredictedLabelColumnOriginalValueConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PredictedLabelColumnOriginalValueConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.PredictedLabelColumnOriginalValueConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GenerateNumberTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GenerateNumberTransformColumn.UseCounter">
            <summary>
            Use an auto-incremented integer starting at zero instead of a random number
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.GenerateNumberTransformColumn.Seed">
            <summary>
            The random seed
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator">
            <summary>
            Adds a column with a generated number sequence.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.Column">
            <summary>
            New column definition(s) (optional form: name:seed)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.UseCounter">
            <summary>
            Use an auto-incremented integer starting at zero instead of a random number
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.Seed">
            <summary>
            The random seed
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RandomNumberGenerator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.RowRangeFilter">
            <summary>
            Filters a dataview on a column of type Single, Double or Key (contiguous). Keeps the values that are in the specified min/max range. NaNs are always filtered out. If the input is a Key type, the min/max are considered percentages of the number of values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Column">
            <summary>
            Column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Min">
            <summary>
            Minimum value (0 to 1 for key types)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Max">
            <summary>
            Maximum value (0 to 1 for key types)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Complement">
            <summary>
            If true, keep the values that fall outside the range.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.IncludeMin">
            <summary>
            If true, include in the range the values that are equal to min.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.IncludeMax">
            <summary>
            If true, include in the range the values that are equal to max.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowRangeFilter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter">
            <summary>
            Allows limiting input to a subset of rows at an optional offset.  Can be used to implement data paging.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter.Skip">
            <summary>
            Number of items to skip
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter.Take">
            <summary>
            Number of items to take
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipAndTakeFilter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.RowSkipFilter">
            <summary>
            Allows limiting input to a subset of rows by skipping a number of rows.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipFilter.Count">
            <summary>
            Number of items to skip
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipFilter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipFilter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowSkipFilter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.RowTakeFilter">
            <summary>
            Allows limiting input to a subset of rows by taking N first rows.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowTakeFilter.Count">
            <summary>
            Number of items to take
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowTakeFilter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowTakeFilter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.RowTakeFilter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.ScoreColumnSelector">
            <summary>
            Selects only the last score columns and the extra columns specified in the arguments.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ScoreColumnSelector.ExtraColumns">
            <summary>
            Extra columns to write
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ScoreColumnSelector.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ScoreColumnSelector.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.ScoreColumnSelector.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.Scorer">
            <summary>
            Turn the predictor model into a transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Scorer.PredictorModel">
            <summary>
            The predictor model to turn into a transform
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Scorer.Output.ScoredData">
            <summary>
            The scored dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Scorer.Output.ScoringTransform">
            <summary>
            The scoring transform
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.Segregator">
            <summary>
        Un-groups vector columns into sequences of rows, inverse of Group transform.
       </summary><remarks>
        <para>This can be thought of as an inverse of the <see cref="T:Microsoft.ML.Transforms.CombinerByContiguousGroupId" />.  
        For all specified vector columns ("pivot" columns), performs the "ungroup" (or "unroll") operation as outlined below.
        </para>
        <para>If the only pivot column is called P, and has size K, then for every row of the input we will produce 
         K rows, that are identical in all columns except P. The column P will become a scalar column, and this 
         column will hold all the original values of input's P, one value per row, in order. The order of columns 
         will remain the same.
        </para>
        <para>Variable-length pivot columns are supported (including zero, which will eliminate the row from the result).</para>
        <para>Multiple pivot columns are also supported:</para>
        <list type="bullet">
          <item><description>A number of output rows is controlled by the 'mode' parameter. 
            <list type="bullet">
              <item><term>outer</term><description> it is equal to the maximum length of pivot columns</description></item>
              <item><term>inner</term><description> it is equal to the minimum length of pivot columns</description></item>
              <item><term>first</term><description> it is equal to the length of the first pivot column</description></item>
            </list>
            </description>
          </item>
          <item><description>
              If a particular pivot column has size that is different than the number of output rows, the extra slots will
              be ignored, and the missing slots will be 'padded' with default values.
            </description></item>
        </list>
        <para>All metadata are preserved for the retained columns. For 'unrolled' columns, all known metadata
        except slot names are preserved.
        </para>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new Segregator
          { 
              Column = new[]{"Column1" },
              Mode = UngroupTransformUngroupMode.First
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Segregator.Column">
            <summary>
            Columns to unroll, or 'pivot'
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Segregator.Mode">
            <summary>
            Specifies how to unroll multiple pivot columns of different size.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Segregator.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Segregator.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.Segregator.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer">
            <summary>
        Uses a pretrained sentiment model to score input strings.
      </summary><remarks>
        <para>The Sentiment transform returns the probability that the sentiment of a natural text is positive. </para>
        <para>
          The model was trained with the <a href="https://anthology.aclweb.org/P/P14/P14-1146.pdf">Sentiment-specific word embedding (SSWE)</a>  and NGramFeaturizer on Twitter sentiment data,
          similarly to the sentiment analysis part of the
          <a href="https://www.microsoft.com/cognitive-services/en-us/text-analytics-api">Text Analytics cognitive service</a>. 
          The transform outputs a score between 0 and 1 as a sentiment prediction 
          (where 0 is a negative sentiment and 1 is a positive sentiment).</para> 
          <para>Currently it supports only English.</para>
      </remarks>
            <!-- No matching elements were found for the following include tag --><include file="../Microsoft.ML.Transforms/Text/doc.xml" path="doc/members/example[@name=&quot;SentimentAnalyzer&quot;]/*" />
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer.Source">
            <summary>
            Name of the source column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer.Name">
            <summary>
            Name of the new column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.SentimentAnalyzer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TensorFlowScorer">
            <summary>
        Extracts hidden layers' values from a pre-trained Tensorflow model.
      </summary><remarks>
        <para>
          The TensorFlowTransform extracts specified outputs using a pre-trained <a href="https://www.tensorflow.org">Tensorflow</a> model.
          The transform takes as inputs the pre-trained Tensorflow model, the names of the input nodes, and names of the output nodes whose values we want to extract. 
        </para>

        <para>
          This transform requires the <a href="https://dotnet.myget.org/feed/dotnet-core/package/nuget/Microsoft.ML.TensorFlow">Microsoft.ML.TensorFlow</a> nuget to be installed.
          The TensorFlowTransform has the following assumptions regarding input, output and processing of data.
        </para>
        <list type="number">
          <item>
            <description>
              For the input model, currently the TensorFlowTransform supports both the <a href="https://www.tensorflow.org/mobile/prepare_models">Frozen model</a> format and also the <a href="https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel">SavedModel</a> format.
            </description>
          </item>
          <item>
            <description>The transform supports scoring only one example at a time.</description>
          </item>
          <item>
            <description>The name of input column(s) should match the name of input(s) in TensorFlow model.</description>
          </item>
          <item>
            <description>The name of each output column should match one of the operations in the TensorFlow graph.</description>
          </item>
          <item>
            <description>Currently, float and double are the only acceptable data types for input/output.</description>
          </item>
          <item>
            <description>Upon success, the transform will introduce a new column in <see cref="T:Microsoft.ML.Runtime.Data.IDataView" /> corresponding to each output column specified.</description>
          </item>
        </list>

        The inputs and outputs of a TensorFlow model can be obtained using the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/README.md#inspecting-graphs">
          <code>summarize_graph</code> tool
        </a>.

      </remarks><example>
        <code language="csharp">
          pipeline.Add(new TextLoader(dataFile).CreateFrom&lt;MNISTData&gt;(useHeader: false));
          pipeline.Add(new ColumnCopier(("NumericImageVec", "Input");
          pipeline.Add(new TensorFlowScorer()
          {
            Model = model_location;
            InputColumns = new[] { "Input" };
            OutputColumns = new[] { "Output" };
          }
        </code>
      </example><example>
        <code language="csharp">
          var pipeline = new Legacy.LearningPipeline(seed: 1);
          pipeline.Add(new Microsoft.ML.Legacy.Data.TextLoader(dataFile).CreateFrom&lt;CifarData&gt;(useHeader: false));
          pipeline.Add(new ImageLoader(("ImagePath", "ImageReal"))
          {
            ImageFolder = imageFolder
          });

          pipeline.Add(new ImageResizer(("ImageReal", "ImageCropped"))
          {
            ImageHeight = imageHeight,
            ImageWidth = imageWidth,
            Resizing = ImageResizerTransformResizingKind.IsoCrop
          });

          pipeline.Add(new ImagePixelExtractor(("ImageCropped", "Input"))
          {
            UseAlpha = false,
            InterleaveArgb = true
          });

          pipeline.Add(new TensorFlowScorer()
          {
            Model = model_location,
            InputColumns = new[] { "Input" },
            OutputColumns = new[] { "Output" }
          });

        </code>
    </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.Model">
            <summary>
            TensorFlow model used by the transform. Please see https://www.tensorflow.org/mobile/prepare_models for more details.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.InputColumns">
            <summary>
            The names of the model inputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.OutputColumns">
            <summary>
            The name of the outputs
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TensorFlowScorer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermLoaderArguments.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermLoaderArguments.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TermLoaderArguments.DropUnknowns">
            <summary>
            Drop unknown terms instead of mapping them to NA term.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TextFeaturizer">
            <summary>
        A transform that turns a collection of text documents into numerical feature vectors.
        The feature vectors are normalized counts of (word and/or character) ngrams in a given tokenized text.
      </summary><remarks>
        The TextFeaturizer transform gives user one-stop solution for doing:
        <list type="bullet">
          <item><description>Language Detection</description></item>
          <item><description>Tokenzation​</description></item>
          <item><description>Text normalization</description></item>
          <item><description>Predefined and custom stopwords removal.</description></item>
          <item><description>Word-based or character-based Ngram and SkipGram extraction.​</description></item>
          <item><description>TF, IDF or TF-IDF.</description></item>
          <item><description>L-p vector normalization.​</description></item>
        </list>
        The TextFeaturizer will show the transformed text, after being applied.
        It converts a collection of text columns to a matrix of token  ngrams/skip-grams counts.
        Features are made of (word/character) n-grams/skip-grams​ and the number of features are equal to the vocabulary size found by analyzing the data.
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new TextFeaturizer("Features", "SentimentText")
          {
            KeepDiacritics = false,
            KeepPunctuations = false,
            TextCase = TextNormalizerTransformCaseNormalizationMode.Lower,
            OutputTokens = true,
            StopWordsRemover = new PredefinedStopWordsRemover(),
            VectorNormalizer = TextTransformTextNormKind.L2,
            CharFeatureExtractor = new NGramNgramExtractor() { NgramLength = 3, AllLengths = false },
            WordFeatureExtractor = new NGramNgramExtractor() { NgramLength = 2, AllLengths = true }
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Column">
            <summary>
            New column definition (optional form: name:srcs).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Language">
            <summary>
            Dataset language or 'AutoDetect' to detect language per row.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.StopWordsRemover">
            <summary>
            Stopwords remover.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.TextCase">
            <summary>
            Casing text using the rules of the invariant culture.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.KeepDiacritics">
            <summary>
            Whether to keep diacritical marks or remove them.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.KeepPunctuations">
            <summary>
            Whether to keep punctuation marks or remove them.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.KeepNumbers">
            <summary>
            Whether to keep numbers or remove them.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.OutputTokens">
            <summary>
            Whether to output the transformed text tokens as an additional column.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Dictionary">
            <summary>
            A dictionary of whitelisted terms.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.WordFeatureExtractor">
            <summary>
            Ngram feature extractor to use for words (WordBag/WordHashBag).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.CharFeatureExtractor">
            <summary>
            Ngram feature extractor to use for characters (WordBag/WordHashBag).
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.VectorNormalizer">
            <summary>
            Normalize vectors (rows) individually by rescaling them to unit norm.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextFeaturizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TextToKeyConverter">
            <summary>
        Converts input values (words, numbers, etc.) to index in a dictionary.
      </summary><remarks>
        The TextToKeyConverter transform builds up term vocabularies (dictionaries).
        The TextToKeyConverter and the <see cref="T:Microsoft.ML.Transforms.HashConverter" /> are the two one primary mechanisms by which raw input is transformed into keys.
        If multiple columns are used, each column builds/uses exactly one vocabulary.
        The output columns are KeyType-valued.
        The Key value is the one-based index of the item in the dictionary.
        If the key is not found in the dictionary, it is assigned the missing value indicator.
        This dictionary mapping values to keys is most commonly learnt from the unique values in input data,
        but can be defined through other means: either with the mapping defined directly on the command line, or as loaded from an external file.
      </remarks><seealso cref="T:Microsoft.ML.Transforms.HashConverter" /><seealso cref="T:Microsoft.ML.Transforms.KeyToTextConverter" />
            <example>
        <code language="csharp">
          pipeline.Add(new TextToKeyConverter(("Column", "OutColumn"))
          { 
              Sort = TermTransformSortOrder.Occurrence 
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.MaxNumTerms">
            <summary>
            Maximum number of terms to keep per column when auto-training
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Term">
            <summary>
            List of terms
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Sort">
            <summary>
            How items should be ordered when vectorized. By default, they will be in the order encountered. If by value items are sorted according to their default comparison, e.g., text sorting will be case sensitive (e.g., 'A' then 'Z' then 'a').
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.TextKeyValues">
            <summary>
            Whether key value metadata should be text, regardless of the actual input type
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TextToKeyConverter.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter">
            <summary>
            Split the dataset into train and test sets
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter.Fraction">
            <summary>
            Fraction of training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter.StratificationColumn">
            <summary>
            Stratification column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter.Output.TrainData">
            <summary>
            Training data
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TrainTestDatasetSplitter.Output.TestData">
            <summary>
            Testing data
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer">
            <summary>
        Trains a tree ensemble, or loads it from a file, then maps a numeric feature vector to outputs.
      </summary><remarks>
        In machine learning​ it is a pretty common and powerful approach to utilize the already trained model in the process of defining features.
        <para>One such example would be the use of model's scores as features to downstream models. For example, we might run clustering on the original features, 
        and use the cluster distances as the new feature set.
        Instead of consuming the model's output, we could go deeper, and extract the 'intermediate outputs' that are used to produce the final score. </para>
        There are a number of famous or popular examples of this technique:
        <list type="bullet">
          <item><description>A deep neural net trained on the ImageNet dataset, with the last layer removed, is commonly used to compute the 'projection' of the image into the 'semantic feature space'.
            It is observed that the Euclidean distance in this space often correlates with the 'semantic similarity': that is, all pictures of pizza are located close together,
            and far away from pictures of kittens. </description></item>
          <item><description>A matrix factorization and/or LDA model is also often used to extract the 'latent topics' or 'latent features' associated with users and items.</description></item>
          <item><description>The weights of the linear model are often used as a crude indicator of 'feature importance'. At the very minimum, the 0-weight features are not needed by the model,
            and there's no reason to compute them. </description></item>
        </list>
        <para>
          Tree featurizer uses the decision tree ensembles for feature engineering in the same fashion as above.
          It trains a tree ensemble, or loads it from a file, then maps a numeric feature vector to three outputs:
        </para>
        <list type="number">
          <item><description>A vector containing the individual tree outputs of the tree ensemble.</description></item>
          <item><description>A vector indicating the leaves that the feature vector falls on in the tree ensemble.</description></item>
          <item><description>A vector indicating the paths that the feature vector falls on in the tree ensemble.</description></item>
        </list>
        If a both a model file and a trainer are specified - will use the model file. If neither are specified,
        will train a default FastTree model.
        This can handle key labels by training a regression model towards their optionally permuted indices.
        <para>Let's assume that we've built a tree ensemble of 100 trees with 100 leaves each (it doesn't matter whether boosting was used or not in training). 
        If we associate each leaf of each tree with a sequential integer, we can, for every incoming example x, 
        produce an indicator vector L(x), where Li(x) = 1 if the example x 'falls' into the leaf #i, and 0 otherwise.</para>
        <para>Thus, for every example x, we produce a 10000-valued vector L, with exactly 100 1s and the rest zeroes. 
        This 'leaf indicator' vector can be considered the ensemble-induced 'footprint' of the example.</para>
        <para>The 'distance' between two examples in the L-space is actually a Hamming distance, and is equal to the number of trees that do not distinguish the two examples.</para>
        <para>We could repeat the same thought process for the non-leaf, or internal, nodes of the trees (we know that each tree has exactly 99 of them in our 100-leaf example), 
        and produce another indicator vector, N (size 9900), for each example, indicating the 'trajectory' of each example through each of the trees.</para>
        <para>The distance in the combined 19900-dimensional LN-space will be equal to the number of 'decisions' in all trees that 'agree' on the given pair of examples.</para>
        <para>The TreeLeafFeaturizer is also producing the third vector, T, which is defined as Ti(x) = output of tree #i on example x.</para>
      </remarks><example>
        <code language="csharp">
          pipeline.Add(new TreeLeafFeaturizer())
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.Suffix">
            <summary>
            Output column: The suffix to append to the default column names
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.LabelPermutationSeed">
            <summary>
            If specified, determines the permutation seed for applying this featurizer to a multiclass problem.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.PredictorModel">
            <summary>
            Trainer to use
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TreeLeafFeaturizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.TwoHeterogeneousModelCombiner">
            <summary>
            Combines a TransformModel and a PredictorModel into a single PredictorModel.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TwoHeterogeneousModelCombiner.TransformModel">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TwoHeterogeneousModelCombiner.PredictorModel">
            <summary>
            Predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.TwoHeterogeneousModelCombiner.Output.PredictorModel">
            <summary>
            Predictor model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ContainsAlpha">
            <summary>
            Whether to use alpha channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ContainsRed">
            <summary>
            Whether to use red channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ContainsGreen">
            <summary>
            Whether to use green channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ContainsBlue">
            <summary>
            Whether to use blue channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.InterleaveArgb">
            <summary>
            Whether to separate each channel or interleave in ARGB order
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ImageWidth">
            <summary>
            Width of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.ImageHeight">
            <summary>
            Height of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.Offset">
            <summary>
            Offset (pre-scale)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.Scale">
            <summary>
            Scale factor
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImageTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.VectorToImage">
            <summary>
            Converts vector array into image type.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ContainsAlpha">
            <summary>
            Whether to use alpha channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ContainsRed">
            <summary>
            Whether to use red channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ContainsGreen">
            <summary>
            Whether to use green channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ContainsBlue">
            <summary>
            Whether to use blue channel
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.InterleaveArgb">
            <summary>
            Whether to separate each channel or interleave in ARGB order
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ImageWidth">
            <summary>
            Width of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.ImageHeight">
            <summary>
            Height of the image
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Offset">
            <summary>
            Offset (pre-scale)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Scale">
            <summary>
            Scale factor
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.VectorToImage.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddingsTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddingsTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.WordEmbeddings">
            <summary>
        Word Embeddings transform is a text featurizer which converts vectors of text tokens into sentence vectors using a pre-trained model.
      </summary><remarks>
        <para>WordEmbeddings wrap different embedding models, such as GloVe. Users can specify which embedding to use. 
        The available options are various versions of <a href="https://nlp.stanford.edu/projects/glove/">GloVe Models</a>, <a href="https://en.wikipedia.org/wiki/FastText">fastText</a>, and <a href="https://anthology.aclweb.org/P/P14/P14-1146.pdf">SSWE</a>.
        </para>
        <para>Note: As WordEmbedding requires a column with text vector, e.g. 'this', 'is', 'good', users need to create an input column by
          using the output_tokens=True for TextTransform to convert a column with sentences like 'This is good' into 'this', 'is', 'good'.
          The suffix of '_TransformedText' is added to the original column name to create the output token column. For instance if the input column is 'body',
          the output tokens column is named 'body_TransformedText'.</para>
        <para>
          License attributes for pretrained models:
          <list type="bullet">
            <item>
              <description>
                "fastText Wikipedia 300D" by Facebook, Inc. is licensed under <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA 3.0</a> based on:
                P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov,<a href="https://arxiv.org/abs/1607.04606">Enriching Word Vectors with Subword Information</a>
                More information can be found <a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">here</a>.
              </description>
            </item>
            <item>
              <description>
                GloVe models by Stanford University, or (Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. 
                <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>) is licensed under <a href="https://opendatacommons.org/licenses/pddl/1.0/">PDDL</a>.
                More information can be found <a href="https://nlp.stanford.edu/projects/glove/">here</a>. 
                Repository can be found <a href="https://github.com/stanfordnlp/GloVe">here</a>.
              </description>
          </item>
        </list>
        </para>
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add(new TextFeaturizer("Words", "InputTextCol")
          {
              TextCase = TextNormalizerTransformCaseNormalizationMode.Lower,
              OutputTokens = true,
              CharFeatureExtractor=null,
              WordFeatureExtractor = null
          });
          pipeline.Add(new WordEmbeddings(("Words_TransformedText", "OutTextCol")));
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.Column">
            <summary>
            New column definition(s) (optional form: name:src)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.ModelKind">
            <summary>
            Pre-trained model used to create the vocabulary
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.CustomLookupTable">
            <summary>
            Filename for custom word embedding model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordEmbeddings.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DelimitedTokenizeTransformColumn.TermSeparators">
            <summary>
            Comma separated set of term separator(s). Commonly: 'space', 'comma', 'semicolon' or other single character.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DelimitedTokenizeTransformColumn.Name">
            <summary>
            Name of the new column
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.DelimitedTokenizeTransformColumn.Source">
            <summary>
            Name of the source column
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.Transforms.WordTokenizer">
            <summary>
        This transform splits the text into words using the separator character(s).
      </summary><remarks>
        The input for this transform is a ReadOnlyMemory or a vector of ReadOnlyMemory,
        and its output is a vector of ReadOnlyMemory, corresponding to the tokens in the input text.
        The output is generated by splitting the input text, using a set of user specified separator characters.
        Empty strings and strings containing only spaces are dropped.
        This transform is not typically used on its own, but it is one of the transforms composing the Text Featurizer.
      </remarks>
            <example>
        <code language="csharp">
          pipeline.Add( new WordTokenizer("TextColumn")
          { 
            TermSeparators = "' ', '\t', ';'"  
          });
        </code>
      </example>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordTokenizer.Column">
            <summary>
            New column definition(s)
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordTokenizer.TermSeparators">
            <summary>
            Comma separated set of term separator(s). Commonly: 'space', 'comma', 'semicolon' or other single character.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordTokenizer.Data">
            <summary>
            Input dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordTokenizer.Output.OutputData">
            <summary>
            Transformed dataset
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.Transforms.WordTokenizer.Output.Model">
            <summary>
            Transform model
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.ILearningPipelineItem">
            <summary>
            An item that can be added to the Learning Pipeline.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.ILearningPipelineItem.GetInputData">
            <summary>
            Returns the place holder for input IDataView object for the node in the execution graph.
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.ILearningPipelineLoader">
            <summary>
            A data loader that can be added to the Learning Pipeline.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.ILearningPipelineStep">
            <summary>
            An item that can be added to the Learning Pipeline that can be trained and or return a IDataView.
            This encapsulates an IDataView(input) and ITransformModel(output) object for a transform and
            for a learner it will encapsulate IDataView(input) and IPredictorModel(output).
            </summary>
        </member>
        <member name="T:Microsoft.ML.Legacy.LearningPipeline">
             <summary>
             The <see cref="T:Microsoft.ML.Legacy.LearningPipeline"/> class is used to define the steps needed to perform a desired machine learning task.<para/>
             The steps are defined by adding a data loader (e.g. <see cref="T:Microsoft.ML.Runtime.Data.TextLoader"/>) followed by zero or more transforms (e.g. <see cref="T:Microsoft.ML.Legacy.Transforms.TextFeaturizer"/>)
             and at most one trainer/learner (e.g. <see cref="T:Microsoft.ML.Legacy.Trainers.FastTreeBinaryClassifier"/>) in the pipeline.
            
             </summary>
             <example>
             <para/>
             For example,<para/>
             <code>
             var pipeline = new LearningPipeline();
             pipeline.Add(new TextLoader &lt;SentimentData&gt; (dataPath, separator: ","));
             pipeline.Add(new TextFeaturizer("Features", "SentimentText"));
             pipeline.Add(new FastTreeBinaryClassifier());
            
             var model = pipeline.Train&lt;SentimentData, SentimentPrediction&gt;();
             </code>
             </example>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.#ctor">
            <summary>
            Construct an empty <see cref="T:Microsoft.ML.Legacy.LearningPipeline"/> object.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.#ctor(System.Nullable{System.Int32},System.Int32)">
            <summary>
             Construct an empty <see cref="T:Microsoft.ML.Legacy.LearningPipeline"/> object.
            </summary>
            <param name="seed">Specify seed for random generator</param>
            <param name="conc">Specify concurrency factor (default value - autoselection)</param>
        </member>
        <member name="P:Microsoft.ML.Legacy.LearningPipeline.Count">
            <summary>
            Get the count of ML components in the <see cref="T:Microsoft.ML.Legacy.LearningPipeline"/> object
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Add(Microsoft.ML.Legacy.ILearningPipelineItem)">
            <summary>
            Add a data loader, transform or trainer into the pipeline.
            Possible data loader(s), transforms and trainers options are
            <para>
            Data Loader:
                <see cref="T:Microsoft.ML.Legacy.Data.TextLoader" />
                etc.
            </para>
            <para>
            Transforms:
                <see cref="T:Microsoft.ML.Legacy.Transforms.Dictionarizer"/>,
                <see cref="T:Microsoft.ML.Legacy.Transforms.CategoricalOneHotVectorizer"/>
                <see cref="T:Microsoft.ML.Legacy.Transforms.MinMaxNormalizer"/>,
                <see cref="T:Microsoft.ML.Legacy.Transforms.ColumnCopier"/>,
                <see cref="T:Microsoft.ML.Legacy.Transforms.ColumnConcatenator"/>,
                <see cref="T:Microsoft.ML.Legacy.Transforms.TextFeaturizer"/>,
                etc.
            </para>
            <para>
            Trainers:
                <see cref="T:Microsoft.ML.Legacy.Trainers.AveragedPerceptronBinaryClassifier"/>,
                <see cref="T:Microsoft.ML.Legacy.Trainers.LogisticRegressionClassifier"/>,
                <see cref="T:Microsoft.ML.Legacy.Trainers.StochasticDualCoordinateAscentClassifier"/>,
                <see cref="T:Microsoft.ML.Legacy.Trainers.FastTreeRegressor"/>,
                etc.
            </para>
            For a complete list of transforms and trainers, please see "Microsoft.ML.Transforms" and "Microsoft.ML.Trainers" namespaces.
            </summary>
            <param name="item">Any ML component (data loader, transform or trainer) defined as <see cref="T:Microsoft.ML.Legacy.ILearningPipelineItem"/>.</param>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Append(Microsoft.ML.Legacy.ILearningPipelineItem)">
            <summary>
            Add a data loader, transform or trainer into the pipeline.
            </summary>
            <param name="item">Any ML component (data loader, transform or trainer) defined as <see cref="T:Microsoft.ML.Legacy.ILearningPipelineItem"/>.</param>
            <returns>Pipeline with added item</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Clear">
            <summary>
            Remove all the loaders/transforms/trainers from the pipeline.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Contains(Microsoft.ML.Legacy.ILearningPipelineItem)">
            <summary>
            Check if a specific loader/transform/trainer is in the pipeline?
            </summary>
            <param name="item">Any ML component (data loader, transform or trainer) defined as <see cref="T:Microsoft.ML.Legacy.ILearningPipelineItem"/>.</param>
            <returns>true if item is found in the pipeline; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.CopyTo(Microsoft.ML.Legacy.ILearningPipelineItem[],System.Int32)">
            <summary>
            Copy the pipeline items into an array.
            </summary>
            <param name="array">The one-dimensional Array that is the destination of the elements copied from.</param>
            <param name="arrayIndex">The zero-based index in <paramref name="array" /> at which copying begins.</param>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Remove(Microsoft.ML.Legacy.ILearningPipelineItem)">
            <summary>
            Remove an item from the pipeline.
            </summary>
            <param name="item"><see cref="T:Microsoft.ML.Legacy.ILearningPipelineItem"/> to remove.</param>
            <returns>true if item was removed from the pipeline; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Train``2">
            <summary>
            Train the model using the ML components in the pipeline.
            </summary>
            <typeparam name="TInput">Type of data instances the model will be trained on. It's a custom type defined by the user according to the structure of data.
            <para/>
            Please see https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/windows for more details on input type.
            </typeparam>
            <typeparam name="TOutput">Ouput type. The prediction will be return based on this type.
            Please see https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/windows for more details on output type.
            </typeparam>
            <returns>PredictionModel object. This is the model object used for prediction on new instances. </returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.LearningPipeline.Execute(Microsoft.ML.Runtime.IHostEnvironment)">
            <summary>
            Executes a pipeline and returns the resulting data.
            </summary>
            <returns>
            The IDataView that was returned by the pipeline.
            </returns>
        </member>
        <member name="T:Microsoft.ML.Legacy.LearningPipelineDebugProxy">
            <summary>
            The debug proxy class for a LearningPipeline.
            Displays the current columns and values in the debugger Watch window.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.LearningPipelineDebugProxy.Columns">
            <summary>
            Gets the column information of the pipeline.
            </summary>
        </member>
        <member name="P:Microsoft.ML.Legacy.LearningPipelineDebugProxy.Rows">
            <summary>
            Gets the row information of the pipeline.
            </summary>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.TryGetScoreLabelNames(System.String[]@,System.String)">
            <summary>
            Returns labels that correspond to indices of the score array in the case of
            multi-class classification problem.
            </summary>
            <param name="names">Label to score mapping</param>
            <param name="scoreColumnName">Name of the score column</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.ReadAsync(System.String)">
            <summary>
            Read model from file asynchronously.
            </summary>
            <param name="path">Path to the file</param>
            <returns>Model</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.ReadAsync(System.IO.Stream)">
            <summary>
            Read model from stream asynchronously.
            </summary>
            <param name="stream">Stream with model</param>
            <returns>Model</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.ReadAsync``2(System.String)">
            <summary>
            Read generic model from file.
            </summary>
            <typeparam name="TInput">Type for incoming data</typeparam>
            <typeparam name="TOutput">Type for output data</typeparam>
            <param name="path">Path to the file</param>
            <returns>Model</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.ReadAsync``2(System.IO.Stream)">
            <summary>
            Read generic model from file.
            </summary>
            <typeparam name="TInput">Type for incoming data</typeparam>
            <typeparam name="TOutput">Type for output data</typeparam>
            <param name="stream">Stream with model</param>
            <returns>Model</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.Predict(Microsoft.ML.Runtime.Data.IDataView)">
            <summary>
            Run prediction on top of IDataView.
            </summary>
            <param name="input">Incoming IDataView</param>
            <returns>IDataView which contains predictions</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.WriteAsync(System.String)">
            <summary>
            Save model to file.
            </summary>
            <param name="path">File to save model</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel.WriteAsync(System.IO.Stream)">
            <summary>
            Save model to stream.
            </summary>
            <param name="stream">Stream to save model.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel`2.Predict(`0)">
            <summary>
            Run prediction for the TInput data.
            </summary>
            <param name="input">Input data</param>
            <returns>Result of prediction</returns>
        </member>
        <member name="M:Microsoft.ML.Legacy.PredictionModel`2.Predict(System.Collections.Generic.IEnumerable{`0})">
            <summary>
            Run prediction for collection of inputs.
            </summary>
            <param name="inputs">Input data</param>
            <returns>Result of prediction</returns>
        </member>
    </members>
</doc>
